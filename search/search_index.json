{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"lzhw (DataFrame Compression) Compression library for data frames and tabular data files, csv, excel etc. Compression library to compress big lists and/or pandas dataframes using an optimized algorithm (lzhw) developed from Lempel-Ziv, Huffman and LZ-Welch techniques. The library supports parallelism and most of its core code is written in Cython so it is quite fast. lzhw has a command line tool that can be downloaded from here and can work from command line with no prior python installation. Manual on how to use it available here . It works on Windows and soon a Mac version will be available. How lzhw Works Overview The library's main goal is to compress data frames, excel and csv files so that they consume less space to overcome memory errors. Also to enable dealing with large files that can cause memory errors when reading them in python or that cause slow operations. With lzhw , we can read compressed files and do operations column by column and on specific rows only on chunks that we are interesred in. The algorithm is a mix of the famous lempel-ziv and huffman coding algorithm with some use of lempel-ziv-welch algorithm. lzhw uses lempel-ziv77 to discover repeated sequences in the stream and construct triplets , in that format < offset,length,literal >. Where offset is how many steps should we return back word to find the beginning of the current sequence and length is how many steps should we move and literal is the next value after the sequence. Then we will have 3 shorter lists representing the stream, where Huffman Coding can come to the game encoding them. .Huffman will produce code dictionaries Then Lempel-Ziv-Welch , lzw_compress() , is used to further compress these dictionaries produces by Huffman. DEFLATE Note The techniques may seem similar to the DEFLATE algorithm which uses both LZSS, which is a variant of LZ77, and huffman coding, but I am not sure how the huffman coding further compresses the triplets. I believe it compresses the triplets altogether not as 3 separate lists as lzhw. And also it doesn't use the lempel-ziv-welch for further compression. DEFLATE Algorithm may be more complicated than lzhw, discussed here, but the latter is designed specifically for tabular data types to help in data science and data analysis projects. Putting all together with LZHW Class All of the steps can be done at once using LZHW class as follows and as shown in the Quick Start section: import lzhw import sys example = [\"to\", \"be\", \"or\", \"not\", \"to\", \"be\", \"or\", \"to\", \"be\", \"or\", \"not\"] * 4 lzhw_comp = lzhw.LZHW(example) print(lzhw_comp.compressed) # (8012, 8012, 15532) # this is how the compressed data looks like and stored print(lzhw_comp.sequences) # {'offset': {3: None, 10: 4, 11: 7, 4: 11}, # 'length': {3: None, 10: 3, 11: 6, 4: 28}, # 'literal_str': {7: 321647, 12: 312421, 13: 319090, 2: 163110516}} Compressed data is now a tuple of three integers and a dictionary of three keys . All values are integers not string for better space saving. print(sys.getsizeof(example)) print(sys.getsizeof(lzhw_comp.sequences) + sys.getsizeof(lzhw_comp.compressed)) # 416 # 312 The compressed tuple and the dictionary sizes are smaller than the original. Try to increase the 4 by which the list is multiplied, you will find that this 312 will never change.","title":"lzhw (DataFrame Compression)"},{"location":"#lzhw-dataframe-compression","text":"Compression library for data frames and tabular data files, csv, excel etc. Compression library to compress big lists and/or pandas dataframes using an optimized algorithm (lzhw) developed from Lempel-Ziv, Huffman and LZ-Welch techniques. The library supports parallelism and most of its core code is written in Cython so it is quite fast. lzhw has a command line tool that can be downloaded from here and can work from command line with no prior python installation. Manual on how to use it available here . It works on Windows and soon a Mac version will be available.","title":"lzhw (DataFrame Compression)"},{"location":"#how-lzhw-works","text":"","title":"How lzhw Works"},{"location":"#overview","text":"The library's main goal is to compress data frames, excel and csv files so that they consume less space to overcome memory errors. Also to enable dealing with large files that can cause memory errors when reading them in python or that cause slow operations. With lzhw , we can read compressed files and do operations column by column and on specific rows only on chunks that we are interesred in. The algorithm is a mix of the famous lempel-ziv and huffman coding algorithm with some use of lempel-ziv-welch algorithm. lzhw uses lempel-ziv77 to discover repeated sequences in the stream and construct triplets , in that format < offset,length,literal >. Where offset is how many steps should we return back word to find the beginning of the current sequence and length is how many steps should we move and literal is the next value after the sequence. Then we will have 3 shorter lists representing the stream, where Huffman Coding can come to the game encoding them. .Huffman will produce code dictionaries Then Lempel-Ziv-Welch , lzw_compress() , is used to further compress these dictionaries produces by Huffman.","title":"Overview"},{"location":"#deflate-note","text":"The techniques may seem similar to the DEFLATE algorithm which uses both LZSS, which is a variant of LZ77, and huffman coding, but I am not sure how the huffman coding further compresses the triplets. I believe it compresses the triplets altogether not as 3 separate lists as lzhw. And also it doesn't use the lempel-ziv-welch for further compression. DEFLATE Algorithm may be more complicated than lzhw, discussed here, but the latter is designed specifically for tabular data types to help in data science and data analysis projects.","title":"DEFLATE Note"},{"location":"#putting-all-together-with-lzhw-class","text":"All of the steps can be done at once using LZHW class as follows and as shown in the Quick Start section: import lzhw import sys example = [\"to\", \"be\", \"or\", \"not\", \"to\", \"be\", \"or\", \"to\", \"be\", \"or\", \"not\"] * 4 lzhw_comp = lzhw.LZHW(example) print(lzhw_comp.compressed) # (8012, 8012, 15532) # this is how the compressed data looks like and stored print(lzhw_comp.sequences) # {'offset': {3: None, 10: 4, 11: 7, 4: 11}, # 'length': {3: None, 10: 3, 11: 6, 4: 28}, # 'literal_str': {7: 321647, 12: 312421, 13: 319090, 2: 163110516}} Compressed data is now a tuple of three integers and a dictionary of three keys . All values are integers not string for better space saving. print(sys.getsizeof(example)) print(sys.getsizeof(lzhw_comp.sequences) + sys.getsizeof(lzhw_comp.compressed)) # 416 # 312 The compressed tuple and the dictionary sizes are smaller than the original. Try to increase the 4 by which the list is multiplied, you will find that this 312 will never change.","title":"Putting all together with LZHW Class"},{"location":"1%20Quick%20Start/","text":"Quick Start Here is a quick walkthrough to get started with the library. Installation pip install lzhw LZHW Class import lzhw sample_data = [\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \"Sunny\", \"Sunny\", \"Rain\", \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Sunny\", \"Sunny\", \"Overcaste\"] compressed = lzhw.LZHW(sample_data) ## let's see how the compressed object looks like: print(compressed.compressed) # (524288, 524288, 81592676324) ## its size print(compressed.size()) # 72 ## size of original from sys import getsizeof print(getsizeof(sample_data)) # 216 print(compressed.space_saving()) # space saving from original to compressed is 67% ## Let's decompress and check whether there is any information loss decomp = compressed.decompress() print(all(decomp == sample_data)) # True As we saw, the LZHW class has saved 67% of the space used to store the original list without any loss. This percentage can get better with bigger data that may have repeated sequences. The class has also some useful helper methods as space_saving , size , and decompress() to revert back to original. Another example with numeric data. from random import sample, choices numbers = choices(sample(range(0, 5), 5), k = 20) comp_num = lzhw.LZHW(numbers) print(getsizeof(numbers) > comp_num.size()) # True print(numbers == list(map(int, comp_num.decompress()))) ## make it int again # True print(comp_num.space_saving()) # space saving from original to compressed is 73% A Long List to 3 Numbers Let's look at how the compressed object is stored and how it looks like when printed: LZHW class has an attribute called compressed which is a tuple of integers representing the encoded triplets. print(comp_num.compressed) # how the compressed is saved (as tuple of 3 integers) # (1048576, 1048576, 43175655208435) Adjusting Sliding Window LZHW class has sliding_window argument which we can adjust in case we want more compression. The sliding window is where the algorithm looks back for previous sequences matching the current location in the input stream. The default value of the argument is 265 meaning that the algorithm will look for matches in the previous 265 values from the current location. We can make this number bigger to allow the algorithm to look for matches in a wider window that can potentially lead to more compression. The algorithm then will behave a little bit slower, you may even not notice it, because lzhw uses hash table, python dictionaries, to look up the matches, i.e. it doesn't loop over the window elements. from time import time from sys import getsizeof sample_data = [\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \"Sunny\", \"Sunny\", \"Rain\", \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Sunny\", \"Sunny\", \"Overcaste\"] * 10000 print(\"Original size:\", getsizeof(sample_data)) # Original size: 1520064 start = time() lzhw265 = lzhw.LZHW(sample_data, sliding_window = 265) print(\"sliding window 265 compression time:\", time() - start) # sliding window 265 compression time: 0.5186636447906494 print(\"sliding window 265 size:\", lzhw265.size()) # sliding window 265 size: 72 start = time() lzhw1024 = lzhw.LZHW(sample_data, sliding_window = 1024) print(\"sliding window 1024 compression time:\", time() - start) # sliding window 1024 compression time: 0.5305755138397217 print(\"sliding window 1024 size:\", lzhw1024.size()) # sliding window 1024 size: 72 Time difference is almost not noticeable, and size is the same because the output is a tuple of 3 integers and the data itself is repeated 10k times so changing the sliding window will not help. Here it is for illustration only but you will notice the difference with real data when you save it into desk. Writing to & Reading from Files We can also write the compressed data to files using save_to_file method, and read it back and decompress it using decompress_from_file function. status = [\"Good\", \"Bad\", \"Bad\", \"Bad\", \"Good\", \"Good\", \"Average\", \"Average\", \"Good\", \"Average\", \"Average\", \"Bad\", \"Average\", \"Good\", \"Bad\", \"Bad\", \"Good\"] comp_status = lzhw.LZHW(status) comp_status.save_to_file(\"status.txt\") decomp_status = lzhw.decompress_from_file(\"status.txt\") print(all(status == decomp_status)) # True","title":"Quick Start"},{"location":"1%20Quick%20Start/#quick-start","text":"Here is a quick walkthrough to get started with the library.","title":"Quick Start"},{"location":"1%20Quick%20Start/#installation","text":"pip install lzhw","title":"Installation"},{"location":"1%20Quick%20Start/#lzhw-class","text":"import lzhw sample_data = [\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \"Sunny\", \"Sunny\", \"Rain\", \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Sunny\", \"Sunny\", \"Overcaste\"] compressed = lzhw.LZHW(sample_data) ## let's see how the compressed object looks like: print(compressed.compressed) # (524288, 524288, 81592676324) ## its size print(compressed.size()) # 72 ## size of original from sys import getsizeof print(getsizeof(sample_data)) # 216 print(compressed.space_saving()) # space saving from original to compressed is 67% ## Let's decompress and check whether there is any information loss decomp = compressed.decompress() print(all(decomp == sample_data)) # True As we saw, the LZHW class has saved 67% of the space used to store the original list without any loss. This percentage can get better with bigger data that may have repeated sequences. The class has also some useful helper methods as space_saving , size , and decompress() to revert back to original. Another example with numeric data. from random import sample, choices numbers = choices(sample(range(0, 5), 5), k = 20) comp_num = lzhw.LZHW(numbers) print(getsizeof(numbers) > comp_num.size()) # True print(numbers == list(map(int, comp_num.decompress()))) ## make it int again # True print(comp_num.space_saving()) # space saving from original to compressed is 73%","title":"LZHW Class"},{"location":"1%20Quick%20Start/#a-long-list-to-3-numbers","text":"Let's look at how the compressed object is stored and how it looks like when printed: LZHW class has an attribute called compressed which is a tuple of integers representing the encoded triplets. print(comp_num.compressed) # how the compressed is saved (as tuple of 3 integers) # (1048576, 1048576, 43175655208435)","title":"A Long List to 3 Numbers"},{"location":"1%20Quick%20Start/#adjusting-sliding-window","text":"LZHW class has sliding_window argument which we can adjust in case we want more compression. The sliding window is where the algorithm looks back for previous sequences matching the current location in the input stream. The default value of the argument is 265 meaning that the algorithm will look for matches in the previous 265 values from the current location. We can make this number bigger to allow the algorithm to look for matches in a wider window that can potentially lead to more compression. The algorithm then will behave a little bit slower, you may even not notice it, because lzhw uses hash table, python dictionaries, to look up the matches, i.e. it doesn't loop over the window elements. from time import time from sys import getsizeof sample_data = [\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \"Sunny\", \"Sunny\", \"Rain\", \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Sunny\", \"Sunny\", \"Overcaste\"] * 10000 print(\"Original size:\", getsizeof(sample_data)) # Original size: 1520064 start = time() lzhw265 = lzhw.LZHW(sample_data, sliding_window = 265) print(\"sliding window 265 compression time:\", time() - start) # sliding window 265 compression time: 0.5186636447906494 print(\"sliding window 265 size:\", lzhw265.size()) # sliding window 265 size: 72 start = time() lzhw1024 = lzhw.LZHW(sample_data, sliding_window = 1024) print(\"sliding window 1024 compression time:\", time() - start) # sliding window 1024 compression time: 0.5305755138397217 print(\"sliding window 1024 size:\", lzhw1024.size()) # sliding window 1024 size: 72 Time difference is almost not noticeable, and size is the same because the output is a tuple of 3 integers and the data itself is repeated 10k times so changing the sliding window will not help. Here it is for illustration only but you will notice the difference with real data when you save it into desk.","title":"Adjusting Sliding Window"},{"location":"1%20Quick%20Start/#writing-to-reading-from-files","text":"We can also write the compressed data to files using save_to_file method, and read it back and decompress it using decompress_from_file function. status = [\"Good\", \"Bad\", \"Bad\", \"Bad\", \"Good\", \"Good\", \"Average\", \"Average\", \"Good\", \"Average\", \"Average\", \"Bad\", \"Average\", \"Good\", \"Bad\", \"Bad\", \"Good\"] comp_status = lzhw.LZHW(status) comp_status.save_to_file(\"status.txt\") decomp_status = lzhw.decompress_from_file(\"status.txt\") print(all(status == decomp_status)) # True","title":"Writing to &amp; Reading from Files"},{"location":"2%20Compressing%20DataFrames/","text":"Compressing DataFrames (in Parallel) From DataFrame to CompressedDF lzhw doesn't work only on lists, it also compress pandas dataframes and save it into compressed files to decompress them later. import pandas as pd df = pd.DataFrame({\"a\": [1, 1, 2, 2, 1, 3, 4, 4], \"b\": [\"A\", \"A\", \"B\", \"B\", \"A\", \"C\", \"D\", \"D\"]}) comp_df = lzhw.CompressedDF(df) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 2003.97it/s] Let's check space saved by compression comp_space = 0 for i in range(len(comp_df.compressed)): comp_space += comp_df.compressed[i].size() print(comp_space, getsizeof(df)) # 296 712 ## Test information loss print(list(map(int, comp_df.compressed[0].decompress())) == list(df.a)) # True Saving and Loading Compressed DataFrames With lzhw we can save a data frame into a compressed file and then read it again using save_to_file method and decompress_df_from_file function. Let's try to decompress in parallel ## Save to file comp_df.save_to_file(\"comp_df.txt\") ## Load the file original = lzhw.decompress_df_from_file(\"comp_df.txt\", parallel = True) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 2004.93it/s] print(original) # a b #0 1 A #1 1 A #2 2 B #3 2 B #4 1 A #5 3 C #6 4 D #7 4 D Compressing Bigger DataFrames Let's try to compress a real-world dataframe german_credit.xlsx file from UCI Machine Learning Repository [1]. Original txt file is 219 KB on desk. gc_original = pd.read_excel(\"examples/german_credit.xlsx\") comp_gc = lzhw.CompressedDF(gc_original, parallel = True, n_jobs = -3) # default value all CPUs but 2 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 257.95it/s] ## Compare sizes in Python: comp_space = 0 for i in range(len(comp_gc.compressed)): comp_space += comp_gc.compressed[i].size() print(comp_space, getsizeof(gc_original)) # 4488 548852 print(list(map(int, comp_gc.compressed[0].decompress())) == list(gc_original.iloc[:, 0])) # True Huge space saving, 99%, with no information loss! Let's now write the compressed dataframe into a file and compare the sizes of the files. comp_gc.save_to_file(\"gc_compressed.txt\") Checking the size of the compressed file, it is 38 KB . Meaning that in total we saved around 82% . Future versions will be optimized to save more space. Let's now check when we reload the file, will we lose any information or not. ## Load the file gc_original2 = lzhw.decompress_df_from_file(\"gc_compressed.txt\") # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 259.46it/s] print(list(map(int, gc_original2.iloc[:, 13])) == list(gc_original.iloc[:, 13])) # True print(gc_original.shape == gc_original2.shape) # True Perfect! There is no information loss at all. We can also adjust the sliding window of the LZ77 algorithm comp_gc512 = lzhw.CompressedDF(gc_original, sliding_window = 512) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 353.21it/s] Sliding window can be very useful in case we want more compressed output. Reference [1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.","title":"Compressing DataFrames (in Parallel)"},{"location":"2%20Compressing%20DataFrames/#compressing-dataframes-in-parallel","text":"","title":"Compressing DataFrames (in Parallel)"},{"location":"2%20Compressing%20DataFrames/#from-dataframe-to-compresseddf","text":"lzhw doesn't work only on lists, it also compress pandas dataframes and save it into compressed files to decompress them later. import pandas as pd df = pd.DataFrame({\"a\": [1, 1, 2, 2, 1, 3, 4, 4], \"b\": [\"A\", \"A\", \"B\", \"B\", \"A\", \"C\", \"D\", \"D\"]}) comp_df = lzhw.CompressedDF(df) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 2003.97it/s] Let's check space saved by compression comp_space = 0 for i in range(len(comp_df.compressed)): comp_space += comp_df.compressed[i].size() print(comp_space, getsizeof(df)) # 296 712 ## Test information loss print(list(map(int, comp_df.compressed[0].decompress())) == list(df.a)) # True","title":"From DataFrame to CompressedDF"},{"location":"2%20Compressing%20DataFrames/#saving-and-loading-compressed-dataframes","text":"With lzhw we can save a data frame into a compressed file and then read it again using save_to_file method and decompress_df_from_file function. Let's try to decompress in parallel ## Save to file comp_df.save_to_file(\"comp_df.txt\") ## Load the file original = lzhw.decompress_df_from_file(\"comp_df.txt\", parallel = True) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 2004.93it/s] print(original) # a b #0 1 A #1 1 A #2 2 B #3 2 B #4 1 A #5 3 C #6 4 D #7 4 D","title":"Saving and Loading Compressed DataFrames"},{"location":"2%20Compressing%20DataFrames/#compressing-bigger-dataframes","text":"Let's try to compress a real-world dataframe german_credit.xlsx file from UCI Machine Learning Repository [1]. Original txt file is 219 KB on desk. gc_original = pd.read_excel(\"examples/german_credit.xlsx\") comp_gc = lzhw.CompressedDF(gc_original, parallel = True, n_jobs = -3) # default value all CPUs but 2 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 257.95it/s] ## Compare sizes in Python: comp_space = 0 for i in range(len(comp_gc.compressed)): comp_space += comp_gc.compressed[i].size() print(comp_space, getsizeof(gc_original)) # 4488 548852 print(list(map(int, comp_gc.compressed[0].decompress())) == list(gc_original.iloc[:, 0])) # True Huge space saving, 99%, with no information loss! Let's now write the compressed dataframe into a file and compare the sizes of the files. comp_gc.save_to_file(\"gc_compressed.txt\") Checking the size of the compressed file, it is 38 KB . Meaning that in total we saved around 82% . Future versions will be optimized to save more space. Let's now check when we reload the file, will we lose any information or not. ## Load the file gc_original2 = lzhw.decompress_df_from_file(\"gc_compressed.txt\") # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 259.46it/s] print(list(map(int, gc_original2.iloc[:, 13])) == list(gc_original.iloc[:, 13])) # True print(gc_original.shape == gc_original2.shape) # True Perfect! There is no information loss at all. We can also adjust the sliding window of the LZ77 algorithm comp_gc512 = lzhw.CompressedDF(gc_original, sliding_window = 512) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 353.21it/s] Sliding window can be very useful in case we want more compressed output.","title":"Compressing Bigger DataFrames"},{"location":"2%20Compressing%20DataFrames/#reference","text":"[1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.","title":"Reference"},{"location":"3%20%28De%29Compressing%20Specific%20columns%20or%20rows%20from%20a%20dataframe/","text":"(De)Compressing specific columns or rows from a dataframe (in Parallel) (De)Compressing in Chunks With lzhw you can choose what columns you are interested in compressing from a data frame. CompressedDF class has an argument selected_cols . import lzhw import pandas as pd gc_original = pd.read_excel(\"examples/german_credit.xlsx\") comp_gc = lzhw.CompressedDF(gc_original, selected_cols = [0, 3, 4, 7]) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 401.11it/s] Also when you have a compressed file that you want to decompress, you don't have to decompress it all, you can choose only specific columns and/or rows to decompress. By this you can deal with large compressed files and do operations column by column quickly and avoid memory errors decompress_df_from_file function has the same argument selected_cols . gc_original2 = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols = [0, 4], parallel = True) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 3348.53it/s] gc_original2.head() # Duration Age #0 6 67 #1 48 22 #2 12 49 #3 42 45 #4 24 53 Let's compare this subset with the original df. gc_original.iloc[:, [0, 4]].head() # Duration Age #0 6 67 #1 48 22 #2 12 49 #3 42 45 #4 24 53 Perfect! We can also select columns by names: gc_subset = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols=[\"Age\", \"Duration\"]) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 6220.92it/s] print(gc_subset.head()) # Duration Age #0 6 67 #1 48 22 #2 12 49 #3 42 45 #4 24 53 selected_cols has \"all\" as its default value. decompress_df_from_file has another argument which is n_rows to specify the number of rows we would like to decompress only. The argument's default value is 0 to decompress all data frame, if specified it will decompress from start until desired number of rows. gc_original_subset = lzhw.decompress_df_from_file(\"gc_compressed.txt\", n_rows = 6) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 914.21it/s] print(gc_original_subset.shape) # (6, 62) This can be very helpful when reading very big data in chunks of rows and columns to avoid MemoryError and to apply operations and online algorithms faster . gc_original_subset_smaller = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols = [1, 4, 8, 9], n_rows = 6) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 3267.86it/s] print(gc_original_subset_smaller.shape) # (6, 4) print(gc_original_subset_smaller) # Amount Age ForeignWorker Class # 0 1169 67 1 Good # 1 5951 22 1 Bad # 2 2096 49 1 Good # 3 7882 45 1 Good # 4 4870 53 1 Bad # 5 9055 35 1 Good","title":"(De)Compressing specific columns or rows from a dataframe (in Parallel)"},{"location":"3%20%28De%29Compressing%20Specific%20columns%20or%20rows%20from%20a%20dataframe/#decompressing-specific-columns-or-rows-from-a-dataframe-in-parallel","text":"","title":"(De)Compressing specific columns or rows from a dataframe (in Parallel)"},{"location":"3%20%28De%29Compressing%20Specific%20columns%20or%20rows%20from%20a%20dataframe/#decompressing-in-chunks","text":"With lzhw you can choose what columns you are interested in compressing from a data frame. CompressedDF class has an argument selected_cols . import lzhw import pandas as pd gc_original = pd.read_excel(\"examples/german_credit.xlsx\") comp_gc = lzhw.CompressedDF(gc_original, selected_cols = [0, 3, 4, 7]) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 401.11it/s] Also when you have a compressed file that you want to decompress, you don't have to decompress it all, you can choose only specific columns and/or rows to decompress. By this you can deal with large compressed files and do operations column by column quickly and avoid memory errors decompress_df_from_file function has the same argument selected_cols . gc_original2 = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols = [0, 4], parallel = True) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 3348.53it/s] gc_original2.head() # Duration Age #0 6 67 #1 48 22 #2 12 49 #3 42 45 #4 24 53 Let's compare this subset with the original df. gc_original.iloc[:, [0, 4]].head() # Duration Age #0 6 67 #1 48 22 #2 12 49 #3 42 45 #4 24 53 Perfect! We can also select columns by names: gc_subset = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols=[\"Age\", \"Duration\"]) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 6220.92it/s] print(gc_subset.head()) # Duration Age #0 6 67 #1 48 22 #2 12 49 #3 42 45 #4 24 53 selected_cols has \"all\" as its default value. decompress_df_from_file has another argument which is n_rows to specify the number of rows we would like to decompress only. The argument's default value is 0 to decompress all data frame, if specified it will decompress from start until desired number of rows. gc_original_subset = lzhw.decompress_df_from_file(\"gc_compressed.txt\", n_rows = 6) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 914.21it/s] print(gc_original_subset.shape) # (6, 62) This can be very helpful when reading very big data in chunks of rows and columns to avoid MemoryError and to apply operations and online algorithms faster . gc_original_subset_smaller = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols = [1, 4, 8, 9], n_rows = 6) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 3267.86it/s] print(gc_original_subset_smaller.shape) # (6, 4) print(gc_original_subset_smaller) # Amount Age ForeignWorker Class # 0 1169 67 1 Good # 1 5951 22 1 Bad # 2 2096 49 1 Good # 3 7882 45 1 Good # 4 4870 53 1 Bad # 5 9055 35 1 Good","title":"(De)Compressing in Chunks"},{"location":"4%20Compressing%20Large%20CSVs%20in%20Chunks/","text":"Compressing Large CSVs in Chunks Compressed Chunks With lzhw we can also compressed a large csv file without needing to read it all in memory using CompressedFromCSV method. It uses pandas chunksize argument to read the file in chunks and compress each chunk and return a dictionary of compressed chunks. We can also specify selected_cols argument to get only specific columns from a file. And parallel argument in case we want to compress each chunk in parallel. Default chunk size is 1 million. So it is preferably to be used with very large files. Let's assume that german Credit [1] data is a big one just for illustration. Because the data is in excel and CompressedFromCSV only works with csv we will change the file into csv first. import pandas as pd gc = pd.read_excel(\"examples/german_credit.xlsx\") gc.to_csv(\"german_credit.csv\", index = False) chunks = gc.shape[0] / 4 ## to have 4 chunks compressed_chunks = lzhw.CompressedFromCSV(\"german_credit.csv\", chunksize = chunks) # Compressing Chunk 0 ... # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 1478.93it/s] # Compressing Chunk 1 ... # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 1515.10it/s] # Compressing Chunk 2 ... # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 1678.66it/s] # Compressing Chunk 3 ... # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 1635.98it/s] # File was compressed in 4 chunk(s) Dictionary of Compressed Chunks We now have a dictionary of four compressed chunks. Let's look at it. ## How many chunks print(compressed_chunks.chunk_ind) # 4 ## Chunk id is the key to get the compressed chunk of data frame print(compressed_chunks.all_comp.keys()) # dict_keys([0, 1, 2, 3]) ## the dicionary print(compressed_chunks.all_comp) # {0: <lzhw.lzhw_df.CompressedDF at 0x23e29ae75c8>, # 1: <lzhw.lzhw_df.CompressedDF at 0x23e2c467f08>, # 2: <lzhw.lzhw_df.CompressedDF at 0x23e29bb7408>, # 3: <lzhw.lzhw_df.CompressedDF at 0x23e29cfb4c8>} As we can see, 4 chunks of 4 CompressedDF class , we can now treat them separately. ## Let's decompress column 0 from chunk 0 and compare it with original 0 column in data gc_chunk00 = compressed_chunks.all_comp[0].compressed[0].decompress() print(all(gc_chunk00 == gc.iloc[:int(chunks), 0])) # because each chunk has a slice of the original dataframe # True Saving and Reading Compressed Chunks Finally, we can save the dictionary to desk using save_to_file method and read it using decompress_df_from_file method. compressed_chunks.save_to_file(\"compressed_chunks.txt\", chunks = \"all\") We can specify the chunks we want to save to file. Default is \"all\". Also while decompressing we can only get specific chunks decomp_chunks = lzhw.decompress_df_from_file(\"compressed_chunks.txt\", selected_chunks = [0, 3]) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 3265.44it/s] # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 3113.33it/s] Each chunk contains a decompressed data frame inside it. Let's check that only two chunks were decompressed: print(decomp_chunks.keys()) # dict_keys([0, 3]) total_rows = 0 for k in decomp_chunks.keys(): total_rows += decomp_chunks[k].shape[0] print(total_rows) # 500 Seems perfect! Data was compressed in 4 chunks 250 rows each, as all data is of 1000 rows. Let's look at the contained data frames print(decomp_chunks[0].iloc[:4, :3]) # Duration Amount InstallmentRatePercentage # 0 6 1169 4 # 1 48 5951 2 # 2 12 2096 2 # 3 42 7882 2 Reference [1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.","title":"Compressing Large CSVs in Chunks"},{"location":"4%20Compressing%20Large%20CSVs%20in%20Chunks/#compressing-large-csvs-in-chunks","text":"","title":"Compressing Large CSVs in Chunks"},{"location":"4%20Compressing%20Large%20CSVs%20in%20Chunks/#compressed-chunks","text":"With lzhw we can also compressed a large csv file without needing to read it all in memory using CompressedFromCSV method. It uses pandas chunksize argument to read the file in chunks and compress each chunk and return a dictionary of compressed chunks. We can also specify selected_cols argument to get only specific columns from a file. And parallel argument in case we want to compress each chunk in parallel. Default chunk size is 1 million. So it is preferably to be used with very large files. Let's assume that german Credit [1] data is a big one just for illustration. Because the data is in excel and CompressedFromCSV only works with csv we will change the file into csv first. import pandas as pd gc = pd.read_excel(\"examples/german_credit.xlsx\") gc.to_csv(\"german_credit.csv\", index = False) chunks = gc.shape[0] / 4 ## to have 4 chunks compressed_chunks = lzhw.CompressedFromCSV(\"german_credit.csv\", chunksize = chunks) # Compressing Chunk 0 ... # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 1478.93it/s] # Compressing Chunk 1 ... # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 1515.10it/s] # Compressing Chunk 2 ... # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 1678.66it/s] # Compressing Chunk 3 ... # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 1635.98it/s] # File was compressed in 4 chunk(s)","title":"Compressed Chunks"},{"location":"4%20Compressing%20Large%20CSVs%20in%20Chunks/#dictionary-of-compressed-chunks","text":"We now have a dictionary of four compressed chunks. Let's look at it. ## How many chunks print(compressed_chunks.chunk_ind) # 4 ## Chunk id is the key to get the compressed chunk of data frame print(compressed_chunks.all_comp.keys()) # dict_keys([0, 1, 2, 3]) ## the dicionary print(compressed_chunks.all_comp) # {0: <lzhw.lzhw_df.CompressedDF at 0x23e29ae75c8>, # 1: <lzhw.lzhw_df.CompressedDF at 0x23e2c467f08>, # 2: <lzhw.lzhw_df.CompressedDF at 0x23e29bb7408>, # 3: <lzhw.lzhw_df.CompressedDF at 0x23e29cfb4c8>} As we can see, 4 chunks of 4 CompressedDF class , we can now treat them separately. ## Let's decompress column 0 from chunk 0 and compare it with original 0 column in data gc_chunk00 = compressed_chunks.all_comp[0].compressed[0].decompress() print(all(gc_chunk00 == gc.iloc[:int(chunks), 0])) # because each chunk has a slice of the original dataframe # True","title":"Dictionary of Compressed Chunks"},{"location":"4%20Compressing%20Large%20CSVs%20in%20Chunks/#saving-and-reading-compressed-chunks","text":"Finally, we can save the dictionary to desk using save_to_file method and read it using decompress_df_from_file method. compressed_chunks.save_to_file(\"compressed_chunks.txt\", chunks = \"all\") We can specify the chunks we want to save to file. Default is \"all\". Also while decompressing we can only get specific chunks decomp_chunks = lzhw.decompress_df_from_file(\"compressed_chunks.txt\", selected_chunks = [0, 3]) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 3265.44it/s] # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 3113.33it/s] Each chunk contains a decompressed data frame inside it. Let's check that only two chunks were decompressed: print(decomp_chunks.keys()) # dict_keys([0, 3]) total_rows = 0 for k in decomp_chunks.keys(): total_rows += decomp_chunks[k].shape[0] print(total_rows) # 500 Seems perfect! Data was compressed in 4 chunks 250 rows each, as all data is of 1000 rows. Let's look at the contained data frames print(decomp_chunks[0].iloc[:4, :3]) # Duration Amount InstallmentRatePercentage # 0 6 1169 4 # 1 48 5951 2 # 2 12 2096 2 # 3 42 7882 2","title":"Saving and Reading Compressed Chunks"},{"location":"4%20Compressing%20Large%20CSVs%20in%20Chunks/#reference","text":"[1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.","title":"Reference"},{"location":"5%20More%20Compression%20Functions/","text":"More Compression Functions Aside from the functions and classes discussed, the library also has some more compression functions that can be used as standalone. lz78() lz78 which performs the famous lempel-ziv78 algorithm which differs from lempel-ziv77 in that instead of triplets it creates a dictionary for the previously seen sequences: import random random.seed(1311) example = random.choices([\"A\", \"B\", \"C\"], k = 20) print(example) #['A', 'A', 'C', 'C', 'A', 'A', 'C', 'C', 'C', 'B', 'B', # 'A', 'B', 'B', 'C', 'C', 'B', 'C', 'C', 'B'] import lzhw lz78_comp, symb_dict = lzhw.lz78(example) print(lz78_comp) # ['1', '1', 'C', '3', '1', 'A', '3', 'C', '3', 'B', # '7', '1', 'B', '7', 'C', '6', 'C', 'C B'] print(symb_dict) # {'A': '1', 'A C': '2', 'C': '3', 'A A': '4', 'C C': '5', # 'C B': '6', 'B': '7', 'A B': '8', 'B C': '9', 'C B C': '10'} huffman_coding() Huffman Coding function which takes a Counter object and encodes each symbol accordingly. from collections import Counter huffs = lzhw.huffman_coding(Counter(example)) print(huffs) # {'A': '10', 'C': '0', 'B': '11'} lzw_compress() and lzw_decompress() They perform lempel-ziv-welch compressing and decompressing print(lzhw.lzw_compress(\"Hello World\")) # 723201696971929295664359987300 print(lzhw.lzw_decompress(lzhw.lzw_compress(\"Hello World\"))) # Hello World lz20() I wanted to modify the lempel-ziv78 and instead of creating a dictionary and returing the codes in the output compressed stream, I wanted to glue the repeated sequences together to get a shorter list with more repeated sequences to further use it with huffman coding. I named this function lempel-ziv20 :D: lz20_ex = lzhw.lz20(example) print(lz20_ex) # ['A', 'A', 'C', 'C', 'A', 'A', 'C', 'C', 'C', 'B', 'B', # 'A', 'B', 'B', 'C', 'C B', 'C', 'C B'] huff20 = lzhw.huffman_coding(Counter(lz20_ex)) print(huff20) # {'A': '10', 'C': '0', 'B': '111', 'C B': '110'} In data with repeated sequences it will give better huffman dictionaries. lz77_compress() and lz77_decompress() The main two functions in the library which apply the lempel-ziv77 algorithm: lz77_ex = lzhw.lz77_compress(example) print(lz77_ex) # [(None, None, b'A') (1, 1, b'C') (1, 1, b'A') (4, 3, b'C') # (None, None, b'B') (1, 1, b'A') (3, 2, b'C') (12, 1, b'B') (15, 2, b'B')] lz77_decomp = lzhw.lz77_decompress(lz77_ex) print(all(lz77_decomp == example)) # True Also we can selected how many elements we want to decompress from the original list instead of decompressing it all: print(lzhw.lz77_decompress(lz77_ex, 3)) # ['A', 'A', 'C'] We can adjust the sliding_window argument in case we want more speed, i.e. lower sliding window, or more compression, i.e. higher sliding window. The sliding window is where the algorithm looks for previous sequences, its default value is 256 meaning that the algorithm will look for matches in 265 previous values from the current location. from sys import getsizeof from time import time random.seed(1191) example = random.choices([\"A\", \"B\", \"C\"], k = 10000) start = time() lz77_ex256 = lzhw.lz77_compress(example, sliding_window = 256) print(time() - start) # 0.0010254383087158203 print(len(lz77_ex256)) # 2136 start = time() lz77_ex1024 = lzhw.lz77_compress(example, sliding_window = 1024) print(time() - start) # 0.003989458084106445 print(len(lz77_ex1024)) # 1769 As you can see the difference in time to look in 4X the default sliding_window is not big, you may not even notice it, because the algorithm uses hash tables to look for sequences instead of looping inside the 1024 window, so it can scale up well. Also worth noting that the difference in the compressed object is somewhat big and the object is smaller. That is advantageous when dealing with large data that we want to control how much compressed we would like it to be.","title":"More Compression Functions"},{"location":"5%20More%20Compression%20Functions/#more-compression-functions","text":"Aside from the functions and classes discussed, the library also has some more compression functions that can be used as standalone.","title":"More Compression Functions"},{"location":"5%20More%20Compression%20Functions/#lz78","text":"lz78 which performs the famous lempel-ziv78 algorithm which differs from lempel-ziv77 in that instead of triplets it creates a dictionary for the previously seen sequences: import random random.seed(1311) example = random.choices([\"A\", \"B\", \"C\"], k = 20) print(example) #['A', 'A', 'C', 'C', 'A', 'A', 'C', 'C', 'C', 'B', 'B', # 'A', 'B', 'B', 'C', 'C', 'B', 'C', 'C', 'B'] import lzhw lz78_comp, symb_dict = lzhw.lz78(example) print(lz78_comp) # ['1', '1', 'C', '3', '1', 'A', '3', 'C', '3', 'B', # '7', '1', 'B', '7', 'C', '6', 'C', 'C B'] print(symb_dict) # {'A': '1', 'A C': '2', 'C': '3', 'A A': '4', 'C C': '5', # 'C B': '6', 'B': '7', 'A B': '8', 'B C': '9', 'C B C': '10'}","title":"lz78()"},{"location":"5%20More%20Compression%20Functions/#huffman_coding","text":"Huffman Coding function which takes a Counter object and encodes each symbol accordingly. from collections import Counter huffs = lzhw.huffman_coding(Counter(example)) print(huffs) # {'A': '10', 'C': '0', 'B': '11'}","title":"huffman_coding()"},{"location":"5%20More%20Compression%20Functions/#lzw_compress-and-lzw_decompress","text":"They perform lempel-ziv-welch compressing and decompressing print(lzhw.lzw_compress(\"Hello World\")) # 723201696971929295664359987300 print(lzhw.lzw_decompress(lzhw.lzw_compress(\"Hello World\"))) # Hello World","title":"lzw_compress() and lzw_decompress()"},{"location":"5%20More%20Compression%20Functions/#lz20","text":"I wanted to modify the lempel-ziv78 and instead of creating a dictionary and returing the codes in the output compressed stream, I wanted to glue the repeated sequences together to get a shorter list with more repeated sequences to further use it with huffman coding. I named this function lempel-ziv20 :D: lz20_ex = lzhw.lz20(example) print(lz20_ex) # ['A', 'A', 'C', 'C', 'A', 'A', 'C', 'C', 'C', 'B', 'B', # 'A', 'B', 'B', 'C', 'C B', 'C', 'C B'] huff20 = lzhw.huffman_coding(Counter(lz20_ex)) print(huff20) # {'A': '10', 'C': '0', 'B': '111', 'C B': '110'} In data with repeated sequences it will give better huffman dictionaries.","title":"lz20()"},{"location":"5%20More%20Compression%20Functions/#lz77_compress-and-lz77_decompress","text":"The main two functions in the library which apply the lempel-ziv77 algorithm: lz77_ex = lzhw.lz77_compress(example) print(lz77_ex) # [(None, None, b'A') (1, 1, b'C') (1, 1, b'A') (4, 3, b'C') # (None, None, b'B') (1, 1, b'A') (3, 2, b'C') (12, 1, b'B') (15, 2, b'B')] lz77_decomp = lzhw.lz77_decompress(lz77_ex) print(all(lz77_decomp == example)) # True Also we can selected how many elements we want to decompress from the original list instead of decompressing it all: print(lzhw.lz77_decompress(lz77_ex, 3)) # ['A', 'A', 'C'] We can adjust the sliding_window argument in case we want more speed, i.e. lower sliding window, or more compression, i.e. higher sliding window. The sliding window is where the algorithm looks for previous sequences, its default value is 256 meaning that the algorithm will look for matches in 265 previous values from the current location. from sys import getsizeof from time import time random.seed(1191) example = random.choices([\"A\", \"B\", \"C\"], k = 10000) start = time() lz77_ex256 = lzhw.lz77_compress(example, sliding_window = 256) print(time() - start) # 0.0010254383087158203 print(len(lz77_ex256)) # 2136 start = time() lz77_ex1024 = lzhw.lz77_compress(example, sliding_window = 1024) print(time() - start) # 0.003989458084106445 print(len(lz77_ex1024)) # 1769 As you can see the difference in time to look in 4X the default sliding_window is not big, you may not even notice it, because the algorithm uses hash tables to look for sequences instead of looping inside the 1024 window, so it can scale up well. Also worth noting that the difference in the compressed object is somewhat big and the object is smaller. That is advantageous when dealing with large data that we want to control how much compressed we would like it to be.","title":"lz77_compress() and lz77_decompress()"},{"location":"6%20Using%20the%20lzhw%20command%20line%20tool/","text":"Using the lzhw Command Line tool In lzhw_cli folder, there is a python script that can work on command line tool to compress and decompress files without having to open it in python. LZHW Compression Tool Also a downloadable exe tool is available in this link . The tool allows to compress and decompress files from and to any form, csv, excel etc without any dependencies or installations. The tool can work in parallel and most of its code is written in Cython, so it is pretty fast . Next page in the documentation there is a comparison in performance with other tools. The tool now works perfectly on Windows and Mac version is being developed. Here is the file and its help argument to see how it works and its arguments: lzhw -h Output usage: lzhw [-h] [-d] -f INPUT -o OUTPUT [-c COLUMNS [COLUMNS ...]] [-r ROWS] [-nh] [-p] [-j JOBS] LZHW is a tabular data compression tool. It is used to compress excel, csv and any flat file. Version: 0.0.10 optional arguments: -h, --help show this help message and exit -d, --decompress decompress input into output -f INPUT, --input INPUT input file to be (de)compressed -o OUTPUT, --output OUTPUT output where to save result -c COLUMNS [COLUMNS ...], --columns COLUMNS [COLUMNS ...] select specific columns by names or indices (1-based) to compress or decompress -r ROWS, --rows ROWS select specific rows to decompress (1-based) -nh, --no-header skip header / data to be compressed has no header -p, --parallel compress or decompress in parallel -j JOBS, --jobs JOBS Number of CPUs to use if parallel (default all but 2) As we can see, the tool takes an input file \"-f\" , and output \"-o\" where it should put the result whether it is compression or decompression based on the optional \"-d\" argument which selects decompression. The tool as well takes a \"-c\" argument which is the Columns in case we want only to compress or decompress specific columns from the input file instead of dealing with all the columns unnecessarily. This argument accepts names and indices separated by coma. The \"-nh\" , --no-header, argument to specify if the data has no header. The \"-r\" , --rows, argument is to specify number of rows to decompress, in case we don't need to decompress all rows. The \"-p\" , --parallel, argument is to make compression and decompression goes in parallel to speed it up. And specifying the \"-j\" , --jobs, argument to determine the number of the CPUs to be used, in default it is all CPUs minus 2. Compress How to compress: The tool can be used through command line. For those who are new to command line, the easiest way to start it is to put the lzhw.exe tool in the same folder with the sheet you want to compress. Then go to the folder's directory at the top where you see the directory path and one click then type cmd , black command line will open to you where you can type the examples below. Using german_credit data from UCI Machine Learning Repository [1] lzhw -f \"german_credit.xlsx\" -o \"gc_comp.txt\" Reading files, Can take 1 minute or something ... Running CScript.exe to convert xls file to csv for better performance Microsoft (R) Windows Script Host Version 5.812 Copyright (C) Microsoft Corporation. All rights reserved. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:02<00:00, 21.92it/s] Finalizing Compression ... Creating gc_comp.txt file ... time taken: 0.06792410214742024 minutes Compressed Successfully In parallel : lzhw -f \"german_credit.xlsx\" -o \"gc_comp.txt\" -p Reading files, Can take 1 minute or something ... Running CScript.exe to convert xls file to csv for better performance Microsoft (R) Windows Script Host Version 5.812 Copyright (C) Microsoft Corporation. All rights reserved. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 74.28it/s] Finalizing Compression ... Creating gc_comp.txt file ... time taken: 0.030775876839955647 minutes Compressed Successfully Now, let's say we are interested only in compressing the Age, Duration and Amount columns lzhw -f \"german_credit.xlsx\" -o \"gc_subset.txt\" -c Age,Duration,Amount Reading files, Can take 1 minute or something ... Running CScript.exe to convert xls file to csv for better performance Microsoft (R) Windows Script Host Version 5.812 Copyright (C) Microsoft Corporation. All rights reserved. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 249.99it/s] Finalizing Compression ... Creating gc_subset.txt file ... time taken: 0.01437713384628296 minutes Compressed Successfully Decompress Now it's time to decompress: If your original excel file was big and of many rows and columns, it's better and faster to decompress it into a csv file instead of excel directly and then save the file as excel if excel type is necessary. This is because python is not that fast in writing data to excel as well as the tool sometimes has \"Corrupted Files\" issues with excel. Decompressing in parallel using 2 CPUs. lzhw -d -f \"gc_comp.txt\" -o \"gc_decompressed.csv\" -p -j 2 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 99.00it/s] Finalizing Decompression ... Creating gc_decompressed.csv file ... time taken: 0.014344350496927897 minutes Decompressed Successfully Look at how the -d argument is used. Let's now check that it was decompressed really successfully: head -n 4 gc_decompressed.csv Duration,Amount,InstallmentRatePercentage,ResidenceDuration,Age,NumberExistingCredits,NumberPeopleMaintenance,Telephone,ForeignWorker,Class,CheckingAccountStatus.lt.0,CheckingAccountStatus.0.to.200,CheckingAccountStatus.gt.200,CheckingAccountStatus.none,CreditHistory.NoCredit.AllPaid,CreditHistory.ThisBank.AllPaid,CreditHistory.PaidDuly,CreditHistory.Delay,CreditHistory.Critical,Purpose.NewCar,Purpose.UsedCar,Purpose.Furniture.Equipment,Purpose.Radio.Television,Purpose.DomesticAppliance,Purpose.Repairs,Purpose.Education,Purpose.Vacation,Purpose.Retraining,Purpose.Business,Purpose.Other,SavingsAccountBonds.lt.100,SavingsAccountBonds.100.to.500,SavingsAccountBonds.500.to.1000,SavingsAccountBonds.gt.1000,SavingsAccountBonds.Unknown,EmploymentDuration.lt.1,EmploymentDuration.1.to.4,EmploymentDuration.4.to.7,EmploymentDuration.gt.7,EmploymentDuration.Unemployed,Personal.Male.Divorced.Seperated,Personal.Female.NotSingle,Personal.Male.Single,Personal.Male.Married.Widowed,Personal.Female.Single,OtherDebtorsGuarantors.None,OtherDebtorsGuarantors.CoApplicant,OtherDebtorsGuarantors.Guarantor,Property.RealEstate,Property.Insurance,Property.CarOther,Property.Unknown,OtherInstallmentPlans.Bank,OtherInstallmentPlans.Stores,OtherInstallmentPlans.None,Housing.Rent,Housing.Own,Housing.ForFree,Job.UnemployedUnskilled,Job.UnskilledResident,Job.SkilledEmployee,Job.Management.SelfEmp.HighlyQualified 6,1169,4,4,67,2,1,0,1,Good,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 48,5951,2,2,22,1,1,1,1,Bad,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 12,2096,2,3,49,1,2,1,1,Good,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,1,0,0 It looks awful in the command line :D but it's decompressed. Now let's say that we only interested in decompressing the first two columns that we don't remember how they were spelled. lzhw -d -f \"gc_comp.txt\" -o \"gc_subset_de.csv\" -c 1,2 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 8.05it/s] Finalizing Decompression ... Creating gc_subset_de.csv file ... time taken: 0.00028291543324788414 minutes Decompressed Successfully Now let's have a look at the decompressed file: head gc_subset_de.csv Duration,Amount 6,1169 48,5951 12,2096 42,7882 24,4870 36,9055 24,2835 36,6948 12,3059 We can also use the -r argument to decompress specific rows from the data frame. lzhw -d -f \"gc_comp.txt\" -o \"gc_subset_de.csv\" -r 4 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 369.69it/s] Finalizing Decompression ... Creating gc_subset_de.csv file ... time taken: 0.007962568600972494 minutes Decompressed Successfully Here we only decompressed the first 4 rows, 1-based, including the header. Let's look how the data looks like: cat \"gc_subset_de.csv\" Duration,Amount,InstallmentRatePercentage,ResidenceDuration,Age,NumberExistingCredits,NumberPeopleMaintenance,Telephone,ForeignWorker,Class,CheckingAccountStatus.lt.0,CheckingAccountStatus.0.to.200,CheckingAccountStatus.gt.200,CheckingAccountStatus.none,CreditHistory.NoCredit.AllPaid,CreditHistory.ThisBank.AllPaid,CreditHistory.PaidDuly,CreditHistory.Delay,CreditHistory.Critical,Purpose.NewCar,Purpose.UsedCar,Purpose.Furniture.Equipment,Purpose.Radio.Television,Purpose.DomesticAppliance,Purpose.Repairs,Purpose.Education,Purpose.Vacation,Purpose.Retraining,Purpose.Business,Purpose.Other,SavingsAccountBonds.lt.100,SavingsAccountBonds.100.to.500,SavingsAccountBonds.500.to.1000,SavingsAccountBonds.gt.1000,SavingsAccountBonds.Unknown,EmploymentDuration.lt.1,EmploymentDuration.1.to.4,EmploymentDuration.4.to.7,EmploymentDuration.gt.7,EmploymentDuration.Unemployed,Personal.Male.Divorced.Seperated,Personal.Female.NotSingle,Personal.Male.Single,Personal.Male.Married.Widowed,Personal.Female.Single,OtherDebtorsGuarantors.None,OtherDebtorsGuarantors.CoApplicant,OtherDebtorsGuarantors.Guarantor,Property.RealEstate,Property.Insurance,Property.CarOther,Property.Unknown,OtherInstallmentPlans.Bank,OtherInstallmentPlans.Stores,OtherInstallmentPlans.None,Housing.Rent,Housing.Own,Housing.ForFree,Job.UnemployedUnskilled,Job.UnskilledResident,Job.SkilledEmployee,Job.Management.SelfEmp.HighlyQualified 6,1169,4,4,67,2,1,0,1,Good,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 48,5951,2,2,22,1,1,1,1,Bad,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 12,2096,2,3,49,1,2,1,1,Good,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,1,0,0 42,7882,2,4,45,1,2,1,1,Good,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,1,0 All data is now 5 rows only including the header. Notes on the Tool 1- compression is much faster than decompression, it is good to compress sequentially and decompress in parallel. 2- This error message can appear while compressing or decompressing in parallel lzhw.exe [-h] [-d] -f INPUT -o OUTPUT [-c COLUMNS [COLUMNS ...]] [-r ROWS] [-nh] lzhw.exe: error: the following arguments are required: -f/--input, -o/--output It is totally fine, just press Enter and proceed or leave it until it tells you \"Compressed Successsfully\" or \"Decompressed Successfully\" . The error is due to some parallelization library bug that has nothing to do with the tool so it is ok. 3- The progress bar of columns compression, it doesn't mean that the tool has finished because it needs still to write the answers. So you need to wait until \"Compressed Successfully\" or \"Decompressed Successfully\" message appears. 4- The tool takes a couple of seconds from 8 to 15 seconds to start working and compressing at the first time and then it runs faster and faster the more you use it. Reference [1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.","title":"Using the lzhw Command Line tool"},{"location":"6%20Using%20the%20lzhw%20command%20line%20tool/#using-the-lzhw-command-line-tool","text":"In lzhw_cli folder, there is a python script that can work on command line tool to compress and decompress files without having to open it in python.","title":"Using the lzhw Command Line tool"},{"location":"6%20Using%20the%20lzhw%20command%20line%20tool/#lzhw-compression-tool","text":"Also a downloadable exe tool is available in this link . The tool allows to compress and decompress files from and to any form, csv, excel etc without any dependencies or installations. The tool can work in parallel and most of its code is written in Cython, so it is pretty fast . Next page in the documentation there is a comparison in performance with other tools. The tool now works perfectly on Windows and Mac version is being developed. Here is the file and its help argument to see how it works and its arguments: lzhw -h Output usage: lzhw [-h] [-d] -f INPUT -o OUTPUT [-c COLUMNS [COLUMNS ...]] [-r ROWS] [-nh] [-p] [-j JOBS] LZHW is a tabular data compression tool. It is used to compress excel, csv and any flat file. Version: 0.0.10 optional arguments: -h, --help show this help message and exit -d, --decompress decompress input into output -f INPUT, --input INPUT input file to be (de)compressed -o OUTPUT, --output OUTPUT output where to save result -c COLUMNS [COLUMNS ...], --columns COLUMNS [COLUMNS ...] select specific columns by names or indices (1-based) to compress or decompress -r ROWS, --rows ROWS select specific rows to decompress (1-based) -nh, --no-header skip header / data to be compressed has no header -p, --parallel compress or decompress in parallel -j JOBS, --jobs JOBS Number of CPUs to use if parallel (default all but 2) As we can see, the tool takes an input file \"-f\" , and output \"-o\" where it should put the result whether it is compression or decompression based on the optional \"-d\" argument which selects decompression. The tool as well takes a \"-c\" argument which is the Columns in case we want only to compress or decompress specific columns from the input file instead of dealing with all the columns unnecessarily. This argument accepts names and indices separated by coma. The \"-nh\" , --no-header, argument to specify if the data has no header. The \"-r\" , --rows, argument is to specify number of rows to decompress, in case we don't need to decompress all rows. The \"-p\" , --parallel, argument is to make compression and decompression goes in parallel to speed it up. And specifying the \"-j\" , --jobs, argument to determine the number of the CPUs to be used, in default it is all CPUs minus 2.","title":"LZHW Compression Tool"},{"location":"6%20Using%20the%20lzhw%20command%20line%20tool/#compress","text":"How to compress: The tool can be used through command line. For those who are new to command line, the easiest way to start it is to put the lzhw.exe tool in the same folder with the sheet you want to compress. Then go to the folder's directory at the top where you see the directory path and one click then type cmd , black command line will open to you where you can type the examples below. Using german_credit data from UCI Machine Learning Repository [1] lzhw -f \"german_credit.xlsx\" -o \"gc_comp.txt\" Reading files, Can take 1 minute or something ... Running CScript.exe to convert xls file to csv for better performance Microsoft (R) Windows Script Host Version 5.812 Copyright (C) Microsoft Corporation. All rights reserved. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:02<00:00, 21.92it/s] Finalizing Compression ... Creating gc_comp.txt file ... time taken: 0.06792410214742024 minutes Compressed Successfully In parallel : lzhw -f \"german_credit.xlsx\" -o \"gc_comp.txt\" -p Reading files, Can take 1 minute or something ... Running CScript.exe to convert xls file to csv for better performance Microsoft (R) Windows Script Host Version 5.812 Copyright (C) Microsoft Corporation. All rights reserved. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 74.28it/s] Finalizing Compression ... Creating gc_comp.txt file ... time taken: 0.030775876839955647 minutes Compressed Successfully Now, let's say we are interested only in compressing the Age, Duration and Amount columns lzhw -f \"german_credit.xlsx\" -o \"gc_subset.txt\" -c Age,Duration,Amount Reading files, Can take 1 minute or something ... Running CScript.exe to convert xls file to csv for better performance Microsoft (R) Windows Script Host Version 5.812 Copyright (C) Microsoft Corporation. All rights reserved. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 249.99it/s] Finalizing Compression ... Creating gc_subset.txt file ... time taken: 0.01437713384628296 minutes Compressed Successfully","title":"Compress"},{"location":"6%20Using%20the%20lzhw%20command%20line%20tool/#decompress","text":"Now it's time to decompress: If your original excel file was big and of many rows and columns, it's better and faster to decompress it into a csv file instead of excel directly and then save the file as excel if excel type is necessary. This is because python is not that fast in writing data to excel as well as the tool sometimes has \"Corrupted Files\" issues with excel. Decompressing in parallel using 2 CPUs. lzhw -d -f \"gc_comp.txt\" -o \"gc_decompressed.csv\" -p -j 2 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 99.00it/s] Finalizing Decompression ... Creating gc_decompressed.csv file ... time taken: 0.014344350496927897 minutes Decompressed Successfully Look at how the -d argument is used. Let's now check that it was decompressed really successfully: head -n 4 gc_decompressed.csv Duration,Amount,InstallmentRatePercentage,ResidenceDuration,Age,NumberExistingCredits,NumberPeopleMaintenance,Telephone,ForeignWorker,Class,CheckingAccountStatus.lt.0,CheckingAccountStatus.0.to.200,CheckingAccountStatus.gt.200,CheckingAccountStatus.none,CreditHistory.NoCredit.AllPaid,CreditHistory.ThisBank.AllPaid,CreditHistory.PaidDuly,CreditHistory.Delay,CreditHistory.Critical,Purpose.NewCar,Purpose.UsedCar,Purpose.Furniture.Equipment,Purpose.Radio.Television,Purpose.DomesticAppliance,Purpose.Repairs,Purpose.Education,Purpose.Vacation,Purpose.Retraining,Purpose.Business,Purpose.Other,SavingsAccountBonds.lt.100,SavingsAccountBonds.100.to.500,SavingsAccountBonds.500.to.1000,SavingsAccountBonds.gt.1000,SavingsAccountBonds.Unknown,EmploymentDuration.lt.1,EmploymentDuration.1.to.4,EmploymentDuration.4.to.7,EmploymentDuration.gt.7,EmploymentDuration.Unemployed,Personal.Male.Divorced.Seperated,Personal.Female.NotSingle,Personal.Male.Single,Personal.Male.Married.Widowed,Personal.Female.Single,OtherDebtorsGuarantors.None,OtherDebtorsGuarantors.CoApplicant,OtherDebtorsGuarantors.Guarantor,Property.RealEstate,Property.Insurance,Property.CarOther,Property.Unknown,OtherInstallmentPlans.Bank,OtherInstallmentPlans.Stores,OtherInstallmentPlans.None,Housing.Rent,Housing.Own,Housing.ForFree,Job.UnemployedUnskilled,Job.UnskilledResident,Job.SkilledEmployee,Job.Management.SelfEmp.HighlyQualified 6,1169,4,4,67,2,1,0,1,Good,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 48,5951,2,2,22,1,1,1,1,Bad,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 12,2096,2,3,49,1,2,1,1,Good,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,1,0,0 It looks awful in the command line :D but it's decompressed. Now let's say that we only interested in decompressing the first two columns that we don't remember how they were spelled. lzhw -d -f \"gc_comp.txt\" -o \"gc_subset_de.csv\" -c 1,2 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 8.05it/s] Finalizing Decompression ... Creating gc_subset_de.csv file ... time taken: 0.00028291543324788414 minutes Decompressed Successfully Now let's have a look at the decompressed file: head gc_subset_de.csv Duration,Amount 6,1169 48,5951 12,2096 42,7882 24,4870 36,9055 24,2835 36,6948 12,3059 We can also use the -r argument to decompress specific rows from the data frame. lzhw -d -f \"gc_comp.txt\" -o \"gc_subset_de.csv\" -r 4 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 369.69it/s] Finalizing Decompression ... Creating gc_subset_de.csv file ... time taken: 0.007962568600972494 minutes Decompressed Successfully Here we only decompressed the first 4 rows, 1-based, including the header. Let's look how the data looks like: cat \"gc_subset_de.csv\" Duration,Amount,InstallmentRatePercentage,ResidenceDuration,Age,NumberExistingCredits,NumberPeopleMaintenance,Telephone,ForeignWorker,Class,CheckingAccountStatus.lt.0,CheckingAccountStatus.0.to.200,CheckingAccountStatus.gt.200,CheckingAccountStatus.none,CreditHistory.NoCredit.AllPaid,CreditHistory.ThisBank.AllPaid,CreditHistory.PaidDuly,CreditHistory.Delay,CreditHistory.Critical,Purpose.NewCar,Purpose.UsedCar,Purpose.Furniture.Equipment,Purpose.Radio.Television,Purpose.DomesticAppliance,Purpose.Repairs,Purpose.Education,Purpose.Vacation,Purpose.Retraining,Purpose.Business,Purpose.Other,SavingsAccountBonds.lt.100,SavingsAccountBonds.100.to.500,SavingsAccountBonds.500.to.1000,SavingsAccountBonds.gt.1000,SavingsAccountBonds.Unknown,EmploymentDuration.lt.1,EmploymentDuration.1.to.4,EmploymentDuration.4.to.7,EmploymentDuration.gt.7,EmploymentDuration.Unemployed,Personal.Male.Divorced.Seperated,Personal.Female.NotSingle,Personal.Male.Single,Personal.Male.Married.Widowed,Personal.Female.Single,OtherDebtorsGuarantors.None,OtherDebtorsGuarantors.CoApplicant,OtherDebtorsGuarantors.Guarantor,Property.RealEstate,Property.Insurance,Property.CarOther,Property.Unknown,OtherInstallmentPlans.Bank,OtherInstallmentPlans.Stores,OtherInstallmentPlans.None,Housing.Rent,Housing.Own,Housing.ForFree,Job.UnemployedUnskilled,Job.UnskilledResident,Job.SkilledEmployee,Job.Management.SelfEmp.HighlyQualified 6,1169,4,4,67,2,1,0,1,Good,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 48,5951,2,2,22,1,1,1,1,Bad,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 12,2096,2,3,49,1,2,1,1,Good,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,1,0,0 42,7882,2,4,45,1,2,1,1,Good,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,1,0 All data is now 5 rows only including the header.","title":"Decompress"},{"location":"6%20Using%20the%20lzhw%20command%20line%20tool/#notes-on-the-tool","text":"1- compression is much faster than decompression, it is good to compress sequentially and decompress in parallel. 2- This error message can appear while compressing or decompressing in parallel lzhw.exe [-h] [-d] -f INPUT -o OUTPUT [-c COLUMNS [COLUMNS ...]] [-r ROWS] [-nh] lzhw.exe: error: the following arguments are required: -f/--input, -o/--output It is totally fine, just press Enter and proceed or leave it until it tells you \"Compressed Successsfully\" or \"Decompressed Successfully\" . The error is due to some parallelization library bug that has nothing to do with the tool so it is ok. 3- The progress bar of columns compression, it doesn't mean that the tool has finished because it needs still to write the answers. So you need to wait until \"Compressed Successfully\" or \"Decompressed Successfully\" message appears. 4- The tool takes a couple of seconds from 8 to 15 seconds to start working and compressing at the first time and then it runs faster and faster the more you use it.","title":"Notes on the Tool"},{"location":"6%20Using%20the%20lzhw%20command%20line%20tool/#reference","text":"[1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.","title":"Reference"},{"location":"7%20Comparing%20LZHW%20with%20Others/","text":"Comparing LZHW with Others I love joblib . I usually use it for parallelism for its great performance coming with a smooth simplicity. I once saw this article in its documentation and it is about measuring the performance between different compressors available in it. Because I am developing a compression library, I wanted to extend the code available in this article adding lzhw to the comparison, just to know where my library stands. joblib uses three main techniques in this article Zlib, LZMA and LZ4 . I will use two data frames here: kddcup99 which is hosted on Machine Learning Repository and 1500000 Sales Records Data . We will look at Compression and Decompression Duration and The compressed file sizes. The downloaded compressed files are 17.5MB and 53MB respectively on the websites Now Let's Begin with the exact code in joblib documentation: KDD Data import os import os.path import time import pandas as pd import lzhw url = (\"https://archive.ics.uci.edu/ml/machine-learning-databases/\" \"kddcup99-mld/kddcup.data.gz\") names = (\"duration, protocol_type, service, flag, src_bytes, \" \"dst_bytes, land, wrong_fragment, urgent, hot, \" \"num_failed_logins, logged_in, num_compromised, \" \"root_shell, su_attempted, num_root, \" \"num_file_creations, Z\").split(', ') data = pd.read_csv(url, names=names, nrows=1e6).reset_index() print(data.shape) # (1000000, 42) We will consider dump as Compression and load as decompression in joblib's algorithms. Now let's see time spent to dump the raw data: from joblib import dump, load pickle_file = './pickle_data.joblib' start = time.time() with open(pickle_file, 'wb') as f: dump(data, f) raw_dump_duration = time.time() - start print(\"Raw dump duration: %0.3fs\" % raw_dump_duration) # Raw dump duration: 0.898s Raw data size: raw_file_size = os.stat(pickle_file).st_size / 1e6 print(\"Raw dump file size: %0.3fMB\" % raw_file_size) # Raw dump file size: 624.035MB Finally, let's measure the time spent to load the dumped raw data: start = time.time() with open(pickle_file, 'rb') as f: load(f) raw_load_duration = time.time() - start print(\"Raw load duration: %0.3fs\" % raw_load_duration) # Raw load duration: 0.900s We will literally do the three steps for all the algorithms and visualize the results and then do the same exercise for the other dataframe Now let's have all the algorithms calls in one code block: ## ZLIB start = time.time() with open(pickle_file, 'wb') as f: dump(data, f, compress='zlib') zlib_dump_duration = time.time() - start print(\"Zlib dump duration: %0.3fs\" % zlib_dump_duration) # Zlib dump duration: 2.362s zlib_file_size = os.stat(pickle_file).st_size / 1e6 print(\"Zlib file size: %0.3fMB\" % zlib_file_size) # Zlib file size: 10.227MB start = time.time() with open(pickle_file, 'rb') as f: load(f) zlib_load_duration = time.time() - start print(\"Zlib load duration: %0.3fs\" % zlib_load_duration) # Zlib load duration: 1.920s ## LZMA start = time.time() with open(pickle_file, 'wb') as f: dump(data, f, compress=('lzma', 3)) lzma_dump_duration = time.time() - start print(\"LZMA dump duration: %0.3fs\" % lzma_dump_duration) # LZMA dump duration: 11.782s lzma_file_size = os.stat(pickle_file).st_size / 1e6 print(\"LZMA file size: %0.3fMB\" % lzma_file_size) # LZMA file size: 4.453MB start = time.time() with open(pickle_file, 'rb') as f: load(f) lzma_load_duration = time.time() - start print(\"LZMA load duration: %0.3fs\" % lzma_load_duration) # LZMA load duration: 2.910s ## LZ4 start = time.time() with open(pickle_file, 'wb') as f: dump(data, f, compress='lz4') lz4_dump_duration = time.time() - start print(\"LZ4 dump duration: %0.3fs\" % lz4_dump_duration) # LZ4 dump duration: 0.723s lz4_file_size = os.stat(pickle_file).st_size / 1e6 print(\"LZ4 file size: %0.3fMB\" % lz4_file_size) # LZ4 file size: 17.693MB start = time.time() with open(pickle_file, 'rb') as f: load(f) lz4_load_duration = time.time() - start print(\"LZ4 load duration: %0.3fs\" % lz4_load_duration) # LZ4 load duration: 1.524s ## LZHW start = time.time() lzhw_data = lzhw.CompressedDF(data, parallel = True, n_jobs = -3) ## all CPUs but 2 lzhw_data.save_to_file(\"lzhw_data.txt\") lzhw_compression_duration = time.time() - start print(\"LZHW compression duration: %0.3fs\" % lzhw_compression_duration) # LZHW compression duration: 37.742s lzhw_file_size = os.stat(\"lzhw_data.txt\").st_size / 1e6 print(\"LZHW file size: %0.3fMB\" % lzhw_file_size) # LZHW file size: 2.839MB start = time.time() lzhw_d = lzhw.decompress_df_from_file(\"lzhw_data.txt\", parallel = True, n_jobs = -3) lzhw_d_duration = time.time() - start print(\"LZHW decompression duration: %0.3fs\" % lzhw_d_duration) # LZHW decompression duration: 9.007s Interesting Results! Let's visualize it: import numpy as np import matplotlib.pyplot as plt N = 5 load_durations = (raw_load_duration, zlib_load_duration, lzma_load_duration, lz4_load_duration, lzhw_d_duration) dump_durations = (raw_dump_duration, zlib_dump_duration, lzma_dump_duration, lz4_dump_duration, lzhw_compression_duration) file_sizes = (raw_file_size, zlib_file_size, lzma_file_size, lz4_file_size, lzhw_file_size) ind = np.arange(N) width = 0.5 plt.figure(1, figsize=(5, 4)) p1 = plt.bar(ind, dump_durations, width) p2 = plt.bar(ind, load_durations, width, bottom=dump_durations) plt.ylabel('Time in seconds') plt.title('Compression & Decompression durations\\nof different algorithms') plt.xticks(ind, ('Raw', 'Zlib', 'LZMA', \"LZ4\", \"LZHW\")) plt.legend((p1[0], p2[0]), ('Compression duration', 'Decompression duration')) The time spent by LZHW to compress seems a little bit higher but let's see the result compressed file: plt.figure(2, figsize=(5, 4)) plt.bar(ind, file_sizes, width, log=True) plt.ylabel('File size in MB') plt.xticks(ind, ('Raw', 'Zlib', 'LZMA', \"LZ4\", \"LZHW\")) plt.title('Compressed data size\\nof different algorithms') for index, value in enumerate(file_sizes): plt.text(index, value, str(round(value)) + \"MB\") LZHW performs much better than all other, while the difference in seconds is not that big 1.5M Sales Data Let's now do the same steps as above on the second data: data = pd.read_csv(\"1500000 Sales Records.csv\") print(data.shape) pickle_file = './pickle_data.joblib' start = time.time() with open(pickle_file, 'wb') as f: dump(data, f) raw_dump_duration = time.time() - start print(\"Raw dump duration: %0.3fs\" % raw_dump_duration) raw_file_size = os.stat(pickle_file).st_size / 1e6 print(\"Raw dump file size: %0.3fMB\" % raw_file_size) start = time.time() with open(pickle_file, 'rb') as f: load(f) raw_load_duration = time.time() - start print(\"Raw load duration: %0.3fs\" % raw_load_duration) ## ZLIB start = time.time() with open(pickle_file, 'wb') as f: dump(data, f, compress='zlib') zlib_dump_duration = time.time() - start print(\"Zlib dump duration: %0.3fs\" % zlib_dump_duration) zlib_file_size = os.stat(pickle_file).st_size / 1e6 print(\"Zlib file size: %0.3fMB\" % zlib_file_size) start = time.time() with open(pickle_file, 'rb') as f: load(f) zlib_load_duration = time.time() - start print(\"Zlib load duration: %0.3fs\" % zlib_load_duration) ## LZMA start = time.time() with open(pickle_file, 'wb') as f: dump(data, f, compress=('lzma', 3)) lzma_dump_duration = time.time() - start print(\"LZMA dump duration: %0.3fs\" % lzma_dump_duration) lzma_file_size = os.stat(pickle_file).st_size / 1e6 print(\"LZMA file size: %0.3fMB\" % lzma_file_size) start = time.time() with open(pickle_file, 'rb') as f: load(f) lzma_load_duration = time.time() - start print(\"LZMA load duration: %0.3fs\" % lzma_load_duration) ## LZ4 start = time.time() with open(pickle_file, 'wb') as f: dump(data, f, compress='lz4') lz4_dump_duration = time.time() - start print(\"LZ4 dump duration: %0.3fs\" % lz4_dump_duration) lz4_file_size = os.stat(pickle_file).st_size / 1e6 print(\"LZ4 file size: %0.3fMB\" % lz4_file_size) start = time.time() with open(pickle_file, 'rb') as f: load(f) lz4_load_duration = time.time() - start print(\"LZ4 load duration: %0.3fs\" % lz4_load_duration) ## LZHW start = time.time() lzhw_data = lzhw.CompressedDF(data) lzhw_data.save_to_file(\"lzhw_data.txt\") lzhw_compression_duration = time.time() - start print(\"LZHW compression duration: %0.3fs\" % lzhw_compression_duration) lzhw_file_size = os.stat(\"lzhw_data.txt\").st_size / 1e6 print(\"LZHW file size: %0.3fMB\" % lzhw_file_size) start = time.time() lzhw_d = lzhw.decompress_df_from_file(\"lzhw_data.txt\", parallel = True, n_jobs = -3) # decompression is slower than compression lzhw_d_duration = time.time() - start print(\"LZHW decompression duration: %0.3fs\" % lzhw_d_duration) # (1500000, 14) # Raw dump duration: 1.294s # Raw dump file size: 267.591MB # Raw load duration: 1.413s # Zlib dump duration: 6.583s # Zlib file size: 96.229MB # Zlib load duration: 2.430s # LZMA dump duration: 76.526s # LZMA file size: 72.476MB # LZMA load duration: 9.240s # LZ4 dump duration: 1.984s # LZ4 file size: 152.374MB # LZ4 load duration: 2.135s # LZHW compression duration: 53.958s # LZHW file size: 41.816MB # LZHW decompression duration: 56.687s Now let's visualize the new results: import numpy as np import matplotlib.pyplot as plt N = 5 load_durations = (raw_load_duration, zlib_load_duration, lzma_load_duration, lz4_load_duration, lzhw_d_duration) dump_durations = (raw_dump_duration, zlib_dump_duration, lzma_dump_duration, lz4_dump_duration, lzhw_compression_duration) file_sizes = (raw_file_size, zlib_file_size, lzma_file_size, lz4_file_size, lzhw_file_size) ind = np.arange(N) width = 0.5 plt.figure(1, figsize=(5, 4)) p1 = plt.bar(ind, dump_durations, width) p2 = plt.bar(ind, load_durations, width, bottom=dump_durations) plt.ylabel('Time in seconds') plt.title('Compression & Decompression durations\\nof different algorithms') plt.xticks(ind, ('Raw', 'Zlib', 'LZMA', \"LZ4\", \"LZHW\")) plt.legend((p1[0], p2[0]), ('Compression duration', 'Decompression duration')) plt.figure(2, figsize=(5, 4)) plt.bar(ind, file_sizes, width, log=True) plt.ylabel('File size in MB') plt.xticks(ind, ('Raw', 'Zlib', 'LZMA', \"LZ4\", \"LZHW\")) plt.title('Compressed data size\\nof different algorithms') for index, value in enumerate(file_sizes): plt.text(index, value, str(round(value)) + \"MB\") By far LZHW outperforms others with acceptable time difference , especially with all other functionalities it enables to deal with compressed data.","title":"Comparing LZHW with Others"},{"location":"7%20Comparing%20LZHW%20with%20Others/#comparing-lzhw-with-others","text":"I love joblib . I usually use it for parallelism for its great performance coming with a smooth simplicity. I once saw this article in its documentation and it is about measuring the performance between different compressors available in it. Because I am developing a compression library, I wanted to extend the code available in this article adding lzhw to the comparison, just to know where my library stands. joblib uses three main techniques in this article Zlib, LZMA and LZ4 . I will use two data frames here: kddcup99 which is hosted on Machine Learning Repository and 1500000 Sales Records Data . We will look at Compression and Decompression Duration and The compressed file sizes. The downloaded compressed files are 17.5MB and 53MB respectively on the websites Now Let's Begin with the exact code in joblib documentation:","title":"Comparing LZHW with Others"},{"location":"7%20Comparing%20LZHW%20with%20Others/#kdd-data","text":"import os import os.path import time import pandas as pd import lzhw url = (\"https://archive.ics.uci.edu/ml/machine-learning-databases/\" \"kddcup99-mld/kddcup.data.gz\") names = (\"duration, protocol_type, service, flag, src_bytes, \" \"dst_bytes, land, wrong_fragment, urgent, hot, \" \"num_failed_logins, logged_in, num_compromised, \" \"root_shell, su_attempted, num_root, \" \"num_file_creations, Z\").split(', ') data = pd.read_csv(url, names=names, nrows=1e6).reset_index() print(data.shape) # (1000000, 42) We will consider dump as Compression and load as decompression in joblib's algorithms. Now let's see time spent to dump the raw data: from joblib import dump, load pickle_file = './pickle_data.joblib' start = time.time() with open(pickle_file, 'wb') as f: dump(data, f) raw_dump_duration = time.time() - start print(\"Raw dump duration: %0.3fs\" % raw_dump_duration) # Raw dump duration: 0.898s Raw data size: raw_file_size = os.stat(pickle_file).st_size / 1e6 print(\"Raw dump file size: %0.3fMB\" % raw_file_size) # Raw dump file size: 624.035MB Finally, let's measure the time spent to load the dumped raw data: start = time.time() with open(pickle_file, 'rb') as f: load(f) raw_load_duration = time.time() - start print(\"Raw load duration: %0.3fs\" % raw_load_duration) # Raw load duration: 0.900s We will literally do the three steps for all the algorithms and visualize the results and then do the same exercise for the other dataframe Now let's have all the algorithms calls in one code block: ## ZLIB start = time.time() with open(pickle_file, 'wb') as f: dump(data, f, compress='zlib') zlib_dump_duration = time.time() - start print(\"Zlib dump duration: %0.3fs\" % zlib_dump_duration) # Zlib dump duration: 2.362s zlib_file_size = os.stat(pickle_file).st_size / 1e6 print(\"Zlib file size: %0.3fMB\" % zlib_file_size) # Zlib file size: 10.227MB start = time.time() with open(pickle_file, 'rb') as f: load(f) zlib_load_duration = time.time() - start print(\"Zlib load duration: %0.3fs\" % zlib_load_duration) # Zlib load duration: 1.920s ## LZMA start = time.time() with open(pickle_file, 'wb') as f: dump(data, f, compress=('lzma', 3)) lzma_dump_duration = time.time() - start print(\"LZMA dump duration: %0.3fs\" % lzma_dump_duration) # LZMA dump duration: 11.782s lzma_file_size = os.stat(pickle_file).st_size / 1e6 print(\"LZMA file size: %0.3fMB\" % lzma_file_size) # LZMA file size: 4.453MB start = time.time() with open(pickle_file, 'rb') as f: load(f) lzma_load_duration = time.time() - start print(\"LZMA load duration: %0.3fs\" % lzma_load_duration) # LZMA load duration: 2.910s ## LZ4 start = time.time() with open(pickle_file, 'wb') as f: dump(data, f, compress='lz4') lz4_dump_duration = time.time() - start print(\"LZ4 dump duration: %0.3fs\" % lz4_dump_duration) # LZ4 dump duration: 0.723s lz4_file_size = os.stat(pickle_file).st_size / 1e6 print(\"LZ4 file size: %0.3fMB\" % lz4_file_size) # LZ4 file size: 17.693MB start = time.time() with open(pickle_file, 'rb') as f: load(f) lz4_load_duration = time.time() - start print(\"LZ4 load duration: %0.3fs\" % lz4_load_duration) # LZ4 load duration: 1.524s ## LZHW start = time.time() lzhw_data = lzhw.CompressedDF(data, parallel = True, n_jobs = -3) ## all CPUs but 2 lzhw_data.save_to_file(\"lzhw_data.txt\") lzhw_compression_duration = time.time() - start print(\"LZHW compression duration: %0.3fs\" % lzhw_compression_duration) # LZHW compression duration: 37.742s lzhw_file_size = os.stat(\"lzhw_data.txt\").st_size / 1e6 print(\"LZHW file size: %0.3fMB\" % lzhw_file_size) # LZHW file size: 2.839MB start = time.time() lzhw_d = lzhw.decompress_df_from_file(\"lzhw_data.txt\", parallel = True, n_jobs = -3) lzhw_d_duration = time.time() - start print(\"LZHW decompression duration: %0.3fs\" % lzhw_d_duration) # LZHW decompression duration: 9.007s Interesting Results! Let's visualize it: import numpy as np import matplotlib.pyplot as plt N = 5 load_durations = (raw_load_duration, zlib_load_duration, lzma_load_duration, lz4_load_duration, lzhw_d_duration) dump_durations = (raw_dump_duration, zlib_dump_duration, lzma_dump_duration, lz4_dump_duration, lzhw_compression_duration) file_sizes = (raw_file_size, zlib_file_size, lzma_file_size, lz4_file_size, lzhw_file_size) ind = np.arange(N) width = 0.5 plt.figure(1, figsize=(5, 4)) p1 = plt.bar(ind, dump_durations, width) p2 = plt.bar(ind, load_durations, width, bottom=dump_durations) plt.ylabel('Time in seconds') plt.title('Compression & Decompression durations\\nof different algorithms') plt.xticks(ind, ('Raw', 'Zlib', 'LZMA', \"LZ4\", \"LZHW\")) plt.legend((p1[0], p2[0]), ('Compression duration', 'Decompression duration')) The time spent by LZHW to compress seems a little bit higher but let's see the result compressed file: plt.figure(2, figsize=(5, 4)) plt.bar(ind, file_sizes, width, log=True) plt.ylabel('File size in MB') plt.xticks(ind, ('Raw', 'Zlib', 'LZMA', \"LZ4\", \"LZHW\")) plt.title('Compressed data size\\nof different algorithms') for index, value in enumerate(file_sizes): plt.text(index, value, str(round(value)) + \"MB\") LZHW performs much better than all other, while the difference in seconds is not that big","title":"KDD Data"},{"location":"7%20Comparing%20LZHW%20with%20Others/#15m-sales-data","text":"Let's now do the same steps as above on the second data: data = pd.read_csv(\"1500000 Sales Records.csv\") print(data.shape) pickle_file = './pickle_data.joblib' start = time.time() with open(pickle_file, 'wb') as f: dump(data, f) raw_dump_duration = time.time() - start print(\"Raw dump duration: %0.3fs\" % raw_dump_duration) raw_file_size = os.stat(pickle_file).st_size / 1e6 print(\"Raw dump file size: %0.3fMB\" % raw_file_size) start = time.time() with open(pickle_file, 'rb') as f: load(f) raw_load_duration = time.time() - start print(\"Raw load duration: %0.3fs\" % raw_load_duration) ## ZLIB start = time.time() with open(pickle_file, 'wb') as f: dump(data, f, compress='zlib') zlib_dump_duration = time.time() - start print(\"Zlib dump duration: %0.3fs\" % zlib_dump_duration) zlib_file_size = os.stat(pickle_file).st_size / 1e6 print(\"Zlib file size: %0.3fMB\" % zlib_file_size) start = time.time() with open(pickle_file, 'rb') as f: load(f) zlib_load_duration = time.time() - start print(\"Zlib load duration: %0.3fs\" % zlib_load_duration) ## LZMA start = time.time() with open(pickle_file, 'wb') as f: dump(data, f, compress=('lzma', 3)) lzma_dump_duration = time.time() - start print(\"LZMA dump duration: %0.3fs\" % lzma_dump_duration) lzma_file_size = os.stat(pickle_file).st_size / 1e6 print(\"LZMA file size: %0.3fMB\" % lzma_file_size) start = time.time() with open(pickle_file, 'rb') as f: load(f) lzma_load_duration = time.time() - start print(\"LZMA load duration: %0.3fs\" % lzma_load_duration) ## LZ4 start = time.time() with open(pickle_file, 'wb') as f: dump(data, f, compress='lz4') lz4_dump_duration = time.time() - start print(\"LZ4 dump duration: %0.3fs\" % lz4_dump_duration) lz4_file_size = os.stat(pickle_file).st_size / 1e6 print(\"LZ4 file size: %0.3fMB\" % lz4_file_size) start = time.time() with open(pickle_file, 'rb') as f: load(f) lz4_load_duration = time.time() - start print(\"LZ4 load duration: %0.3fs\" % lz4_load_duration) ## LZHW start = time.time() lzhw_data = lzhw.CompressedDF(data) lzhw_data.save_to_file(\"lzhw_data.txt\") lzhw_compression_duration = time.time() - start print(\"LZHW compression duration: %0.3fs\" % lzhw_compression_duration) lzhw_file_size = os.stat(\"lzhw_data.txt\").st_size / 1e6 print(\"LZHW file size: %0.3fMB\" % lzhw_file_size) start = time.time() lzhw_d = lzhw.decompress_df_from_file(\"lzhw_data.txt\", parallel = True, n_jobs = -3) # decompression is slower than compression lzhw_d_duration = time.time() - start print(\"LZHW decompression duration: %0.3fs\" % lzhw_d_duration) # (1500000, 14) # Raw dump duration: 1.294s # Raw dump file size: 267.591MB # Raw load duration: 1.413s # Zlib dump duration: 6.583s # Zlib file size: 96.229MB # Zlib load duration: 2.430s # LZMA dump duration: 76.526s # LZMA file size: 72.476MB # LZMA load duration: 9.240s # LZ4 dump duration: 1.984s # LZ4 file size: 152.374MB # LZ4 load duration: 2.135s # LZHW compression duration: 53.958s # LZHW file size: 41.816MB # LZHW decompression duration: 56.687s Now let's visualize the new results: import numpy as np import matplotlib.pyplot as plt N = 5 load_durations = (raw_load_duration, zlib_load_duration, lzma_load_duration, lz4_load_duration, lzhw_d_duration) dump_durations = (raw_dump_duration, zlib_dump_duration, lzma_dump_duration, lz4_dump_duration, lzhw_compression_duration) file_sizes = (raw_file_size, zlib_file_size, lzma_file_size, lz4_file_size, lzhw_file_size) ind = np.arange(N) width = 0.5 plt.figure(1, figsize=(5, 4)) p1 = plt.bar(ind, dump_durations, width) p2 = plt.bar(ind, load_durations, width, bottom=dump_durations) plt.ylabel('Time in seconds') plt.title('Compression & Decompression durations\\nof different algorithms') plt.xticks(ind, ('Raw', 'Zlib', 'LZMA', \"LZ4\", \"LZHW\")) plt.legend((p1[0], p2[0]), ('Compression duration', 'Decompression duration')) plt.figure(2, figsize=(5, 4)) plt.bar(ind, file_sizes, width, log=True) plt.ylabel('File size in MB') plt.xticks(ind, ('Raw', 'Zlib', 'LZMA', \"LZ4\", \"LZHW\")) plt.title('Compressed data size\\nof different algorithms') for index, value in enumerate(file_sizes): plt.text(index, value, str(round(value)) + \"MB\") By far LZHW outperforms others with acceptable time difference , especially with all other functionalities it enables to deal with compressed data.","title":"1.5M Sales Data"}]}