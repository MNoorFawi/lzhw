{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"lzhw (DataFrame Compression) Compression library for data frames and tabular data files, csv, excel etc. Compression library to compress big lists and/or pandas dataframes using an optimized algorithm (lzhw) developed from Lempel-Ziv, Huffman and LZ-Welch techniques. lzhw has a command line tool that can be downloaded from here and can work from command line with no prior python installation. Manual on how to use it available here . It works on Windows and soon a Mac version will be available. How lzhw Works Overview The library's main goal is to compress data frames, excel and csv files so that they consume less space to overcome memory errors. Also to enable dealing with large files that can cause memory errors when reading them in python or that cause slow operations. With lzhw , we can read compressed files and do operations column by column only on columns that we are interesred in. The algorithm is a mix of the famous lempel-ziv and huffman coding algorithm with some use of lempel-ziv-welch algorithm. The algorithm starts with an input stream for example this one: example = [\"to\", \"be\", \"or\", \"not\", \"to\", \"be\", \"or\", \"to\", \"be\", \"or\", \"not\"] * 2 print(\"\".join(example)) # tobeornottobeortobeornottobeornottobeortobeornot Lempel-Ziv77 with Huffman Coding lzhw uses lempel-ziv77 to discover repeated sequences in the stream and construct triplets , in that format < offset,length,literal >. Where offset is how many steps should we return back word to find the beginning of the current sequence and length is how many steps should we move and literal is the next value after the sequence. Then we will have 3 shorter lists representing the stream, where Huffman Coding can come to the game encoding them. The function that performs lempel-ziv and returning the triplets called lz77_compress . import lzhw lz77_ex = lzhw.lz77_compress(example) print(lz77_ex) # [(None, None, 'to'), (None, None, 'be'), (None, None, 'or'), # (None, None, 'not'), (4, 3, 'to'), (7, 6, 'not'), (11, 6, 'not')] Here all the None s values are originally \"0s\" but converted to None to save more space. Now huffman coding will take the offsets list, lengths list and literal list and encode them based on most occurring values to give: lz77_lists = list(zip(*lz77_ex)) print(lz77_lists) # [(None, None, None, None, 4, 7, 11), # (None, None, None, None, 3, 6, 6), # ('to', 'be', 'or', 'not', 'to', 'not', 'not')] huffs = [] from collections import Counter for i in range(len(lz77_lists)): huff = lzhw.huffman_coding(Counter(lz77_lists[i])) huffs.append(huff) print(huffs) # [{None: '1', 4: '010', 7: '011', 11: '00'}, {None: '1', 3: '00', 6: '01'}, # {'to': '11', 'be': '100', 'or': '101', 'not': '0'}] Now if we encode each value in the triplets with its corresponding value from the huffman dictionary and append everything together we will have: bits = [] for i in range(len(huffs)): bit = \"\".join([huffs[i].get(k) for k in lz77_lists[i]]) bits.append(bit) print(bits) # ['111101001100', '1111000101', '1110010101100'] print(len(\"\".join(bits))) # 35 Which has a length of 35 bits only! Then Lempel-Ziv-Welch , lzw_compress() , is used to further compress the dictionaries produces by Huffman. Better than Huffman Alone Using each algorithm alone can give us bigger number of bits, for example, using only huffman coding will give us: huff_alone = lzhw.huffman_coding(Counter(example)) print(huff_alone) # {'to': '11', 'be': '01', 'or': '10', 'not': '00'} huff_bit = \"\".join([huff_alone.get(k) for k in example]) print(huff_bit) # 11011000110110110110001101100011011011011000 print(len(huff_bit)) # 44 44 bits, 9 more bit!!! Big deal when dealing with bigger data. DEFLATE Note The techniques may seem similar to the DEFLATE algorithm which uses both LZSS, which is a variant of LZ77, and huffman coding, but I am not sure how the huffman coding further compresses the triplets. I believe it compresses the triplets altogether not as 3 separate lists as lzhw. And also it doesn't use the lempel-ziv-welch for further compression. DEFLATE Algorithm may be more complicated than lzhw, discussed here, but the latter is designed specifically for tabular data types to help in data science and data analysis projects. Putting all together with LZHW Class All of the steps can be done at once using LZHW class as follows and as shown in the Quick Start section: lzhw_comp = lzhw.LZHW(example) print(lzhw_comp.compressed) # (8012, 1989, 15532) # this is how the compressed data looks like and stored print(lzhw_comp.sequences) # {'offset': {3: None, 10: 4, 11: 7, 4: 11}, # 'length': {3: None, 4: 3, 5: 6}, # 'literal_str': {7: 321647, 12: 312421, 13: 319090, 2: 163110516}}","title":"lzhw (DataFrame Compression)"},{"location":"#lzhw-dataframe-compression","text":"Compression library for data frames and tabular data files, csv, excel etc. Compression library to compress big lists and/or pandas dataframes using an optimized algorithm (lzhw) developed from Lempel-Ziv, Huffman and LZ-Welch techniques. lzhw has a command line tool that can be downloaded from here and can work from command line with no prior python installation. Manual on how to use it available here . It works on Windows and soon a Mac version will be available.","title":"lzhw (DataFrame Compression)"},{"location":"#how-lzhw-works","text":"","title":"How lzhw Works"},{"location":"#overview","text":"The library's main goal is to compress data frames, excel and csv files so that they consume less space to overcome memory errors. Also to enable dealing with large files that can cause memory errors when reading them in python or that cause slow operations. With lzhw , we can read compressed files and do operations column by column only on columns that we are interesred in. The algorithm is a mix of the famous lempel-ziv and huffman coding algorithm with some use of lempel-ziv-welch algorithm. The algorithm starts with an input stream for example this one: example = [\"to\", \"be\", \"or\", \"not\", \"to\", \"be\", \"or\", \"to\", \"be\", \"or\", \"not\"] * 2 print(\"\".join(example)) # tobeornottobeortobeornottobeornottobeortobeornot","title":"Overview"},{"location":"#lempel-ziv77-with-huffman-coding","text":"lzhw uses lempel-ziv77 to discover repeated sequences in the stream and construct triplets , in that format < offset,length,literal >. Where offset is how many steps should we return back word to find the beginning of the current sequence and length is how many steps should we move and literal is the next value after the sequence. Then we will have 3 shorter lists representing the stream, where Huffman Coding can come to the game encoding them. The function that performs lempel-ziv and returning the triplets called lz77_compress . import lzhw lz77_ex = lzhw.lz77_compress(example) print(lz77_ex) # [(None, None, 'to'), (None, None, 'be'), (None, None, 'or'), # (None, None, 'not'), (4, 3, 'to'), (7, 6, 'not'), (11, 6, 'not')] Here all the None s values are originally \"0s\" but converted to None to save more space. Now huffman coding will take the offsets list, lengths list and literal list and encode them based on most occurring values to give: lz77_lists = list(zip(*lz77_ex)) print(lz77_lists) # [(None, None, None, None, 4, 7, 11), # (None, None, None, None, 3, 6, 6), # ('to', 'be', 'or', 'not', 'to', 'not', 'not')] huffs = [] from collections import Counter for i in range(len(lz77_lists)): huff = lzhw.huffman_coding(Counter(lz77_lists[i])) huffs.append(huff) print(huffs) # [{None: '1', 4: '010', 7: '011', 11: '00'}, {None: '1', 3: '00', 6: '01'}, # {'to': '11', 'be': '100', 'or': '101', 'not': '0'}] Now if we encode each value in the triplets with its corresponding value from the huffman dictionary and append everything together we will have: bits = [] for i in range(len(huffs)): bit = \"\".join([huffs[i].get(k) for k in lz77_lists[i]]) bits.append(bit) print(bits) # ['111101001100', '1111000101', '1110010101100'] print(len(\"\".join(bits))) # 35 Which has a length of 35 bits only! Then Lempel-Ziv-Welch , lzw_compress() , is used to further compress the dictionaries produces by Huffman.","title":"Lempel-Ziv77 with Huffman Coding"},{"location":"#better-than-huffman-alone","text":"Using each algorithm alone can give us bigger number of bits, for example, using only huffman coding will give us: huff_alone = lzhw.huffman_coding(Counter(example)) print(huff_alone) # {'to': '11', 'be': '01', 'or': '10', 'not': '00'} huff_bit = \"\".join([huff_alone.get(k) for k in example]) print(huff_bit) # 11011000110110110110001101100011011011011000 print(len(huff_bit)) # 44 44 bits, 9 more bit!!! Big deal when dealing with bigger data.","title":"Better than Huffman Alone"},{"location":"#deflate-note","text":"The techniques may seem similar to the DEFLATE algorithm which uses both LZSS, which is a variant of LZ77, and huffman coding, but I am not sure how the huffman coding further compresses the triplets. I believe it compresses the triplets altogether not as 3 separate lists as lzhw. And also it doesn't use the lempel-ziv-welch for further compression. DEFLATE Algorithm may be more complicated than lzhw, discussed here, but the latter is designed specifically for tabular data types to help in data science and data analysis projects.","title":"DEFLATE Note"},{"location":"#putting-all-together-with-lzhw-class","text":"All of the steps can be done at once using LZHW class as follows and as shown in the Quick Start section: lzhw_comp = lzhw.LZHW(example) print(lzhw_comp.compressed) # (8012, 1989, 15532) # this is how the compressed data looks like and stored print(lzhw_comp.sequences) # {'offset': {3: None, 10: 4, 11: 7, 4: 11}, # 'length': {3: None, 4: 3, 5: 6}, # 'literal_str': {7: 321647, 12: 312421, 13: 319090, 2: 163110516}}","title":"Putting all together with LZHW Class"},{"location":"1%20Quick%20Start/","text":"Quick Start Here is a quick walkthrough to get started with the library. Installation pip install lzhw LZHW Class import lzhw sample_data = [\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \"Sunny\", \"Sunny\", \"Rain\", \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Sunny\", \"Sunny\", \"Overcaste\"] compressed = lzhw.LZHW(sample_data) ## let's see how the compressed object looks like: print(compressed.compressed) # (506460, 128794, 112504) ## its size print(compressed.size()) # 72 ## size of original from sys import getsizeof print(getsizeof(sample_data)) # 216 print(compressed.space_saving()) # space saving from original to compressed is 67% ## Let's decompress and check whether there is any information loss decomp = compressed.decompress() print(decomp == sample_data) # True As we saw, the LZHW class has saved 67% of the space used to store the original list without any loss. This percentage can get better with bigger data that may have repeated sequences. The class has also some useful helper methods as space_saving , size , and decompress() to revert back to original. Another example with numeric data. from random import sample, choices numbers = choices(sample(range(0, 5), 5), k = 20) comp_num = lzhw.LZHW(numbers) print(getsizeof(numbers) > comp_num.size()) # True print(numbers == list(map(int, comp_num.decompress()))) ## make it int again # True print(comp_num.space_saving()) # space saving from original to compressed is 73% A Long List to 3 Numbers Let's look at how the compressed object is stored and how it looks like when printed: LZHW class has an attribute called compressed which is a tuple of integers representing the encoded triplets. print(comp_num.compressed) # how the compressed is saved (as tuple of 3 integers) # (8198555, 620206, 3059308) Writing to & Reading from Files We can also write the compressed data to files using save_to_file method, and read it back and decompress it using decompress_from_file function. status = [\"Good\", \"Bad\", \"Bad\", \"Bad\", \"Good\", \"Good\", \"Average\", \"Average\", \"Good\", \"Average\", \"Average\", \"Bad\", \"Average\", \"Good\", \"Bad\", \"Bad\", \"Good\"] comp_status = lzhw.LZHW(status) comp_status.save_to_file(\"status.txt\") decomp_status = lzhw.decompress_from_file(\"status.txt\") print(status == decomp_status) # True","title":"Quick Start"},{"location":"1%20Quick%20Start/#quick-start","text":"Here is a quick walkthrough to get started with the library.","title":"Quick Start"},{"location":"1%20Quick%20Start/#installation","text":"pip install lzhw","title":"Installation"},{"location":"1%20Quick%20Start/#lzhw-class","text":"import lzhw sample_data = [\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \"Sunny\", \"Sunny\", \"Rain\", \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Sunny\", \"Sunny\", \"Overcaste\"] compressed = lzhw.LZHW(sample_data) ## let's see how the compressed object looks like: print(compressed.compressed) # (506460, 128794, 112504) ## its size print(compressed.size()) # 72 ## size of original from sys import getsizeof print(getsizeof(sample_data)) # 216 print(compressed.space_saving()) # space saving from original to compressed is 67% ## Let's decompress and check whether there is any information loss decomp = compressed.decompress() print(decomp == sample_data) # True As we saw, the LZHW class has saved 67% of the space used to store the original list without any loss. This percentage can get better with bigger data that may have repeated sequences. The class has also some useful helper methods as space_saving , size , and decompress() to revert back to original. Another example with numeric data. from random import sample, choices numbers = choices(sample(range(0, 5), 5), k = 20) comp_num = lzhw.LZHW(numbers) print(getsizeof(numbers) > comp_num.size()) # True print(numbers == list(map(int, comp_num.decompress()))) ## make it int again # True print(comp_num.space_saving()) # space saving from original to compressed is 73%","title":"LZHW Class"},{"location":"1%20Quick%20Start/#a-long-list-to-3-numbers","text":"Let's look at how the compressed object is stored and how it looks like when printed: LZHW class has an attribute called compressed which is a tuple of integers representing the encoded triplets. print(comp_num.compressed) # how the compressed is saved (as tuple of 3 integers) # (8198555, 620206, 3059308)","title":"A Long List to 3 Numbers"},{"location":"1%20Quick%20Start/#writing-to-reading-from-files","text":"We can also write the compressed data to files using save_to_file method, and read it back and decompress it using decompress_from_file function. status = [\"Good\", \"Bad\", \"Bad\", \"Bad\", \"Good\", \"Good\", \"Average\", \"Average\", \"Good\", \"Average\", \"Average\", \"Bad\", \"Average\", \"Good\", \"Bad\", \"Bad\", \"Good\"] comp_status = lzhw.LZHW(status) comp_status.save_to_file(\"status.txt\") decomp_status = lzhw.decompress_from_file(\"status.txt\") print(status == decomp_status) # True","title":"Writing to &amp; Reading from Files"},{"location":"2%20Compressing%20DataFrames/","text":"Compressing DataFrames From DataFrame to CompressedDF lzhw doesn't work only on lists, it also compress pandas dataframes and save it into compressed files to decompress them later. import pandas as pd df = pd.DataFrame({\"a\": [1, 1, 2, 2, 1, 3, 4, 4], \"b\": [\"A\", \"A\", \"B\", \"B\", \"A\", \"C\", \"D\", \"D\"]}) comp_df = lzhw.CompressedDF(df) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 2003.97it/s] Let's check space saved by compression comp_space = 0 for i in range(len(comp_df.compressed)): comp_space += comp_df.compressed[i].size() print(comp_space, getsizeof(df)) # 144 712 ## Test information loss print(comp_df.compressed[0].decompress() == list(map(str, df.a))) # True Saving and Loading Compressed DataFrames With lzhw we can save a data frame into a compressed file and then read it again using save_to_file method and decompress_df_from_file function. ## Save to file comp_df.save_to_file(\"comp_df.txt\") ## Load the file original = lzhw.decompress_df_from_file(\"comp_df.txt\") # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 2004.93it/s] print(original) # a b #0 1 A #1 1 A #2 2 B #3 2 B #4 1 A #5 3 C #6 4 D #7 4 D Compressing Bigger DataFrames Let's try to compress a real-world dataframe german_credit.xlsx file from UCI Machine Learning Repository [1]. Original txt file is 219 KB on desk. gc_original = pd.read_excel(\"examples/german_credit.xlsx\") comp_gc = lzhw.CompressedDF(gc_original) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 257.95it/s] ## Compare sizes in Python: comp_space = 0 for i in range(len(comp_gc.compressed)): comp_space += comp_gc.compressed[i].size() print(comp_space, getsizeof(gc_original)) # 4504 548852 print(comp_gc.compressed[0].decompress() == list(map(str, gc_original.iloc[:, 0]))) # True Huge space saving, 99%, with no information loss! Let's now write the compressed dataframe into a file and compare the sizes of the files. comp_gc.save_to_file(\"gc_compressed.txt\") Checking the size of the compressed file, it is 44 KB . Meaning that in total we saved around 79% . Future versions will be optimized to save more space. Let's now check when we reload the file, will we lose any information or not. ## Load the file gc_original2 = lzhw.decompress_df_from_file(\"gc_compressed.txt\") # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 259.46it/s] print(list(gc_original2.iloc[:, 13]) == list(map(str, gc_original.iloc[:, 13]))) # True print(gc_original.shape == gc_original2.shape) # True Perfect! There is no information loss at all. Reference [1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.","title":"Compressing DataFrames"},{"location":"2%20Compressing%20DataFrames/#compressing-dataframes","text":"","title":"Compressing DataFrames"},{"location":"2%20Compressing%20DataFrames/#from-dataframe-to-compresseddf","text":"lzhw doesn't work only on lists, it also compress pandas dataframes and save it into compressed files to decompress them later. import pandas as pd df = pd.DataFrame({\"a\": [1, 1, 2, 2, 1, 3, 4, 4], \"b\": [\"A\", \"A\", \"B\", \"B\", \"A\", \"C\", \"D\", \"D\"]}) comp_df = lzhw.CompressedDF(df) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 2003.97it/s] Let's check space saved by compression comp_space = 0 for i in range(len(comp_df.compressed)): comp_space += comp_df.compressed[i].size() print(comp_space, getsizeof(df)) # 144 712 ## Test information loss print(comp_df.compressed[0].decompress() == list(map(str, df.a))) # True","title":"From DataFrame to CompressedDF"},{"location":"2%20Compressing%20DataFrames/#saving-and-loading-compressed-dataframes","text":"With lzhw we can save a data frame into a compressed file and then read it again using save_to_file method and decompress_df_from_file function. ## Save to file comp_df.save_to_file(\"comp_df.txt\") ## Load the file original = lzhw.decompress_df_from_file(\"comp_df.txt\") # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 2004.93it/s] print(original) # a b #0 1 A #1 1 A #2 2 B #3 2 B #4 1 A #5 3 C #6 4 D #7 4 D","title":"Saving and Loading Compressed DataFrames"},{"location":"2%20Compressing%20DataFrames/#compressing-bigger-dataframes","text":"Let's try to compress a real-world dataframe german_credit.xlsx file from UCI Machine Learning Repository [1]. Original txt file is 219 KB on desk. gc_original = pd.read_excel(\"examples/german_credit.xlsx\") comp_gc = lzhw.CompressedDF(gc_original) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 257.95it/s] ## Compare sizes in Python: comp_space = 0 for i in range(len(comp_gc.compressed)): comp_space += comp_gc.compressed[i].size() print(comp_space, getsizeof(gc_original)) # 4504 548852 print(comp_gc.compressed[0].decompress() == list(map(str, gc_original.iloc[:, 0]))) # True Huge space saving, 99%, with no information loss! Let's now write the compressed dataframe into a file and compare the sizes of the files. comp_gc.save_to_file(\"gc_compressed.txt\") Checking the size of the compressed file, it is 44 KB . Meaning that in total we saved around 79% . Future versions will be optimized to save more space. Let's now check when we reload the file, will we lose any information or not. ## Load the file gc_original2 = lzhw.decompress_df_from_file(\"gc_compressed.txt\") # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 259.46it/s] print(list(gc_original2.iloc[:, 13]) == list(map(str, gc_original.iloc[:, 13]))) # True print(gc_original.shape == gc_original2.shape) # True Perfect! There is no information loss at all.","title":"Compressing Bigger DataFrames"},{"location":"2%20Compressing%20DataFrames/#reference","text":"[1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.","title":"Reference"},{"location":"3%20%28De%29Compressing%20Specific%20columns%20or%20rows%20from%20a%20dataframe/","text":"(De)Compressing specific columns or rows from a dataframe (De)Compressing in Chunks With lzhw you can choose what columns you are interested in compressing from a data frame. CompressedDF class has an argument selected_cols . import lzhw import pandas as pd gc_original = pd.read_excel(\"examples/german_credit.xlsx\") comp_gc = lzhw.CompressedDF(gc_original, selected_cols = [0, 3, 4, 7]) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 401.11it/s] Also when you have a compressed file that you want to decompress, you don't have to decompress it all, you can choose only specific columns and/or rows to decompress. By this you can deal with large compressed files and do operations column by column quickly and avoid memory errors decompress_df_from_file function has the same argument selected_cols . gc_original2 = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols = [0, 4]) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 3348.53it/s] gc_original2.head() # Duration Age #0 6 67 #1 48 22 #2 12 49 #3 42 45 #4 24 53 Let's compare this subset with the original df. gc_original.iloc[:, [0, 4]].head() # Duration Age #0 6 67 #1 48 22 #2 12 49 #3 42 45 #4 24 53 Perfect! We can also select columns by names: gc_subset = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols=[\"Age\", \"Duration\"]) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 6220.92it/s] print(gc_subset.head()) # Duration Age #0 6 67 #1 48 22 #2 12 49 #3 42 45 #4 24 53 selected_cols has \"all\" as its default value. decompress_df_from_file has another argument which is n_rows to specify the number of rows we would like to decompress only. The argument's default value is 0 to decompress all data frame, if specified it will decompress from start until desired number of rows. gc_original_subset = lzhw.decompress_df_from_file(\"gc_compressed.txt\", n_rows = 6) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 914.21it/s] print(gc_original_subset.shape) # (6, 62) This can be very helpful when reading very big data in chunks of rows and columns to avoid MemoryError and to apply operations and online algorithms faster . gc_original_subset_smaller = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols = [1, 4, 8, 9], n_rows = 6) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 3267.86it/s] print(gc_original_subset_smaller.shape) # (6, 4) print(gc_original_subset_smaller) # Amount Age ForeignWorker Class # 0 1169 67 1 Good # 1 5951 22 1 Bad # 2 2096 49 1 Good # 3 7882 45 1 Good # 4 4870 53 1 Bad # 5 9055 35 1 Good","title":"(De)Compressing specific columns or rows from a dataframe"},{"location":"3%20%28De%29Compressing%20Specific%20columns%20or%20rows%20from%20a%20dataframe/#decompressing-specific-columns-or-rows-from-a-dataframe","text":"","title":"(De)Compressing specific columns or rows from a dataframe"},{"location":"3%20%28De%29Compressing%20Specific%20columns%20or%20rows%20from%20a%20dataframe/#decompressing-in-chunks","text":"With lzhw you can choose what columns you are interested in compressing from a data frame. CompressedDF class has an argument selected_cols . import lzhw import pandas as pd gc_original = pd.read_excel(\"examples/german_credit.xlsx\") comp_gc = lzhw.CompressedDF(gc_original, selected_cols = [0, 3, 4, 7]) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 401.11it/s] Also when you have a compressed file that you want to decompress, you don't have to decompress it all, you can choose only specific columns and/or rows to decompress. By this you can deal with large compressed files and do operations column by column quickly and avoid memory errors decompress_df_from_file function has the same argument selected_cols . gc_original2 = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols = [0, 4]) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 3348.53it/s] gc_original2.head() # Duration Age #0 6 67 #1 48 22 #2 12 49 #3 42 45 #4 24 53 Let's compare this subset with the original df. gc_original.iloc[:, [0, 4]].head() # Duration Age #0 6 67 #1 48 22 #2 12 49 #3 42 45 #4 24 53 Perfect! We can also select columns by names: gc_subset = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols=[\"Age\", \"Duration\"]) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 6220.92it/s] print(gc_subset.head()) # Duration Age #0 6 67 #1 48 22 #2 12 49 #3 42 45 #4 24 53 selected_cols has \"all\" as its default value. decompress_df_from_file has another argument which is n_rows to specify the number of rows we would like to decompress only. The argument's default value is 0 to decompress all data frame, if specified it will decompress from start until desired number of rows. gc_original_subset = lzhw.decompress_df_from_file(\"gc_compressed.txt\", n_rows = 6) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 914.21it/s] print(gc_original_subset.shape) # (6, 62) This can be very helpful when reading very big data in chunks of rows and columns to avoid MemoryError and to apply operations and online algorithms faster . gc_original_subset_smaller = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols = [1, 4, 8, 9], n_rows = 6) # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 3267.86it/s] print(gc_original_subset_smaller.shape) # (6, 4) print(gc_original_subset_smaller) # Amount Age ForeignWorker Class # 0 1169 67 1 Good # 1 5951 22 1 Bad # 2 2096 49 1 Good # 3 7882 45 1 Good # 4 4870 53 1 Bad # 5 9055 35 1 Good","title":"(De)Compressing in Chunks"},{"location":"4%20More%20Compression%20Functions/","text":"More Compression Functions Aside from the functions and classes discussed, the library also has some more compression functions that can be used as standalone. lz78() lz78 which performs the famous lempel-ziv78 algorithm which differs from lempel-ziv77 in that instead of triplets it creates a dictionary for the previously seen sequences: import random random.seed(1311) example = random.choices([\"A\", \"B\", \"C\"], k = 20) print(example) #['A', 'A', 'C', 'C', 'A', 'A', 'C', 'C', 'C', 'B', 'B', # 'A', 'B', 'B', 'C', 'C', 'B', 'C', 'C', 'B'] import lzhw lz78_comp, symb_dict = lzhw.lz78(example) print(lz78_comp) # ['1', '1', 'C', '3', '1', 'A', '3', 'C', '3', 'B', # '7', '1', 'B', '7', 'C', '6', 'C', 'C B'] print(symb_dict) # {'A': '1', 'A C': '2', 'C': '3', 'A A': '4', 'C C': '5', # 'C B': '6', 'B': '7', 'A B': '8', 'B C': '9', 'C B C': '10'} huffman_coding() Huffman Coding function which takes a Counter object and encodes each symbol accordingly. from collections import Counter huffs = lzhw.huffman_coding(Counter(example)) print(huffs) # {'A': '10', 'C': '0', 'B': '11'} lzw_compress() and lzw_decompress() They perform lempel-ziv-welch compressing and decompressing print(lzhw.lzw_compress(\"Hello World\")) # 723201696971929295664359987300 print(lzhw.lzw_decompress(lzhw.lzw_compress(\"Hello World\"))) # Hello World lz20() I wanted to modify the lempel-ziv78 and instead of creating a dictionary and returing the codes in the output compressed stream, I wanted to glue the repeated sequences together to get a shorter list with more repeated sequences to further use it with huffman coding. I named this function lempel-ziv20 :D: lz20_ex = lzhw.lz20(example) print(lz20_ex) # ['A', 'A', 'C', 'C', 'A', 'A', 'C', 'C', 'C', 'B', 'B', # 'A', 'B', 'B', 'C', 'C B', 'C', 'C B'] huff20 = lzhw.huffman_coding(Counter(lz20_ex)) print(huff20) # {'A': '10', 'C': '0', 'B': '111', 'C B': '110'} In data with repeated sequences it will give better huffman dictionaries. lz77_compress() and lz77_decompress() The main two functions in the library which apply the lempel-ziv77 algorithm: lz77_ex = lzhw.lz77_compress(example) print(lz77_ex) # [(None, None, 'A'), (1, 1, 'C'), (1, 1, 'A'), (4, 3, 'C'), # (None, None, 'B'), (1, 1, 'A'), (3, 2, 'C'), (7, 2, 'C'), (1, 1, 'B')] lz77_decomp = lzhw.lz77_decompress(lz77_ex) print(lz77_decomp == example) # True Also we can selected how many elements we want to decompress from the original list instead of decompressing it all: print(lzhw.lz77_decompress(lz77_ex, 3)) #['A', 'A', 'C']","title":"More Compression Functions"},{"location":"4%20More%20Compression%20Functions/#more-compression-functions","text":"Aside from the functions and classes discussed, the library also has some more compression functions that can be used as standalone.","title":"More Compression Functions"},{"location":"4%20More%20Compression%20Functions/#lz78","text":"lz78 which performs the famous lempel-ziv78 algorithm which differs from lempel-ziv77 in that instead of triplets it creates a dictionary for the previously seen sequences: import random random.seed(1311) example = random.choices([\"A\", \"B\", \"C\"], k = 20) print(example) #['A', 'A', 'C', 'C', 'A', 'A', 'C', 'C', 'C', 'B', 'B', # 'A', 'B', 'B', 'C', 'C', 'B', 'C', 'C', 'B'] import lzhw lz78_comp, symb_dict = lzhw.lz78(example) print(lz78_comp) # ['1', '1', 'C', '3', '1', 'A', '3', 'C', '3', 'B', # '7', '1', 'B', '7', 'C', '6', 'C', 'C B'] print(symb_dict) # {'A': '1', 'A C': '2', 'C': '3', 'A A': '4', 'C C': '5', # 'C B': '6', 'B': '7', 'A B': '8', 'B C': '9', 'C B C': '10'}","title":"lz78()"},{"location":"4%20More%20Compression%20Functions/#huffman_coding","text":"Huffman Coding function which takes a Counter object and encodes each symbol accordingly. from collections import Counter huffs = lzhw.huffman_coding(Counter(example)) print(huffs) # {'A': '10', 'C': '0', 'B': '11'}","title":"huffman_coding()"},{"location":"4%20More%20Compression%20Functions/#lzw_compress-and-lzw_decompress","text":"They perform lempel-ziv-welch compressing and decompressing print(lzhw.lzw_compress(\"Hello World\")) # 723201696971929295664359987300 print(lzhw.lzw_decompress(lzhw.lzw_compress(\"Hello World\"))) # Hello World","title":"lzw_compress() and lzw_decompress()"},{"location":"4%20More%20Compression%20Functions/#lz20","text":"I wanted to modify the lempel-ziv78 and instead of creating a dictionary and returing the codes in the output compressed stream, I wanted to glue the repeated sequences together to get a shorter list with more repeated sequences to further use it with huffman coding. I named this function lempel-ziv20 :D: lz20_ex = lzhw.lz20(example) print(lz20_ex) # ['A', 'A', 'C', 'C', 'A', 'A', 'C', 'C', 'C', 'B', 'B', # 'A', 'B', 'B', 'C', 'C B', 'C', 'C B'] huff20 = lzhw.huffman_coding(Counter(lz20_ex)) print(huff20) # {'A': '10', 'C': '0', 'B': '111', 'C B': '110'} In data with repeated sequences it will give better huffman dictionaries.","title":"lz20()"},{"location":"4%20More%20Compression%20Functions/#lz77_compress-and-lz77_decompress","text":"The main two functions in the library which apply the lempel-ziv77 algorithm: lz77_ex = lzhw.lz77_compress(example) print(lz77_ex) # [(None, None, 'A'), (1, 1, 'C'), (1, 1, 'A'), (4, 3, 'C'), # (None, None, 'B'), (1, 1, 'A'), (3, 2, 'C'), (7, 2, 'C'), (1, 1, 'B')] lz77_decomp = lzhw.lz77_decompress(lz77_ex) print(lz77_decomp == example) # True Also we can selected how many elements we want to decompress from the original list instead of decompressing it all: print(lzhw.lz77_decompress(lz77_ex, 3)) #['A', 'A', 'C']","title":"lz77_compress() and lz77_decompress()"},{"location":"5%20Using%20the%20lzhw%20command%20line%20tool/","text":"Using the lzhw Command Line tool In lzhw_cli folder, there is a python script that can work on command line tool to compress and decompress files without having to open it in python. LZHW Compression Tool Also a downloadable exe tool is available in this link . The tool allows to compress and decompress files from and to any form, csv,xlsx etc without any dependencies or installations. The tool now works perfectly on Windows and Mac version is being developed. Here is the file and its help argument to see how it works and its arguments: lzhw -h Output usage: lzhw_cli.py [-h] [-d] -f INPUT -o OUTPUT [-c COLUMNS [COLUMNS ...]] [-r ROWS] [-nh] LZHW is a tabular data compression tool. It is used to compress excel, csv and any flat file. Version: 0.0.8 optional arguments: -h, --help show this help message and exit -d, --decompress decompress input into output -f INPUT, --input INPUT input file to be (de)compressed -o OUTPUT, --output OUTPUT output where to save result -c COLUMNS [COLUMNS ...], --columns COLUMNS [COLUMNS ...] select specific columns by names or indices (1-based) to compress or decompress -r ROWS, --rows ROWS select specific rows to decompress (1-based) -nh, --no-header skip header / data to be compressed has no header As we can see, the tool takes an input file \"-f\" , and output \"-o\" where it should put the result whether it is compression or decompression based on the optional \"-d\" argument which selects decompression. The tool as well takes a \"-c\" argument which is the Columns in case we want only to compress or decompress specific columns from the input file instead of dealing with all the columns unnecessarily. This argument accepts names and indices seperated by coma. The \"-nh\" , --no-header, argument to specify if the data has no header. The \"-r\" , --rows, argument is to specify number of rows to decompress, in case we don't need to decompress all rows. Compress How to compress: The tool can be used through command line. For those who are new to command line, the easiest way to start it is to put the lzhw.exe tool in the same folder with the sheet you want to compress. Then go to the folder's directory at the top where you see the directory path and one click then type cmd , black command line will open to you where you can type the examples below. lzhw -f \"german_credit.xlsx\" -o \"gc_comp.txt\" Reading files, Can take 1 minute or something ... Running CScript.exe to convert xls file to csv for better performance Microsoft (R) Windows Script Host Version 5.812 Copyright (C) Microsoft Corporation. All rights reserved. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 647.30it/s] Creating gc_comp.txt file ... Compressed Successfully Let's say we are interested only in compressing the Age, Duration and Amount columns lzhw -f \"german_credit.xlsx\" -o \"gc_subset.txt\" -c Age,Duration,Amount Reading files, Can take 1 minute or something ... Running CScript.exe to convert xls file to csv for better performance Microsoft (R) Windows Script Host Version 5.812 Copyright (C) Microsoft Corporation. All rights reserved. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 249.99it/s] Creating gc_subset.txt file ... Compressed Successfully Decompress Now it's time to decompress: If your original excel file was big and of many rows and columns, it's better and faster to decompress it into a csv file instead of excel directly and then save the file as excel if excel type is necessary. This is because python is not that fast in writing data to excel as well as the tool sometimes has \"Corrupted Files\" issues with excel. lzhw -d -f \"gc_comp.txt\" -o \"gc_decompressed.csv\" 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 690.45it/s] Creating gc_decompressed.csv file ... Decompressed Successfully Look at how the -d argument is used. Let's now check that it was decompressed really successfully: head -n 4 gc_decompressed.csv Duration,Amount,InstallmentRatePercentage,ResidenceDuration,Age,NumberExistingCredits,NumberPeopleMaintenance,Telephone,ForeignWorker,Class,CheckingAccountStatus.lt.0,CheckingAccountStatus.0.to.200,CheckingAccountStatus.gt.200,CheckingAccountStatus.none,CreditHistory.NoCredit.AllPaid,CreditHistory.ThisBank.AllPaid,CreditHistory.PaidDuly,CreditHistory.Delay,CreditHistory.Critical,Purpose.NewCar,Purpose.UsedCar,Purpose.Furniture.Equipment,Purpose.Radio.Television,Purpose.DomesticAppliance,Purpose.Repairs,Purpose.Education,Purpose.Vacation,Purpose.Retraining,Purpose.Business,Purpose.Other,SavingsAccountBonds.lt.100,SavingsAccountBonds.100.to.500,SavingsAccountBonds.500.to.1000,SavingsAccountBonds.gt.1000,SavingsAccountBonds.Unknown,EmploymentDuration.lt.1,EmploymentDuration.1.to.4,EmploymentDuration.4.to.7,EmploymentDuration.gt.7,EmploymentDuration.Unemployed,Personal.Male.Divorced.Seperated,Personal.Female.NotSingle,Personal.Male.Single,Personal.Male.Married.Widowed,Personal.Female.Single,OtherDebtorsGuarantors.None,OtherDebtorsGuarantors.CoApplicant,OtherDebtorsGuarantors.Guarantor,Property.RealEstate,Property.Insurance,Property.CarOther,Property.Unknown,OtherInstallmentPlans.Bank,OtherInstallmentPlans.Stores,OtherInstallmentPlans.None,Housing.Rent,Housing.Own,Housing.ForFree,Job.UnemployedUnskilled,Job.UnskilledResident,Job.SkilledEmployee,Job.Management.SelfEmp.HighlyQualified 6,1169,4,4,67,2,1,0,1,Good,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 48,5951,2,2,22,1,1,1,1,Bad,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 12,2096,2,3,49,1,2,1,1,Good,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,1,0,0 It looks awful in the command line :D but it's decompressed. Now let's say that we only interested in decompressing the first two columns that we don't remember how they were spelled. lzhw -d -f \"gc_comp.txt\" -o \"gc_subset_de.csv\" -c 1,2 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 5651.84it/s] Creating gc_subset_de.csv file ... Decompressed Successfully Now let's have a look at the decompressed file: head gc_subset_de.csv Duration,Amount 6,1169 48,5951 12,2096 42,7882 24,4870 36,9055 24,2835 36,6948 12,3059 We can also use the -r argument to decompress specific rows from the data frame. lzhw -d -f \"gc_comp.txt\" -o \"gc_subset_de.csv\" -r 4 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 369.69it/s] Creating gc_subset_de.csv file ... Decompressed Successfully Here we only decompressed the firt 4 rows, 1-based, including the header. Let's look how the data looks like: cat \"gc_subset_de.csv\" Duration,Amount,InstallmentRatePercentage,ResidenceDuration,Age,NumberExistingCredits,NumberPeopleMaintenance,Telephone,ForeignWorker,Class,CheckingAccountStatus.lt.0,CheckingAccountStatus.0.to.200,CheckingAccountStatus.gt.200,CheckingAccountStatus.none,CreditHistory.NoCredit.AllPaid,CreditHistory.ThisBank.AllPaid,CreditHistory.PaidDuly,CreditHistory.Delay,CreditHistory.Critical,Purpose.NewCar,Purpose.UsedCar,Purpose.Furniture.Equipment,Purpose.Radio.Television,Purpose.DomesticAppliance,Purpose.Repairs,Purpose.Education,Purpose.Vacation,Purpose.Retraining,Purpose.Business,Purpose.Other,SavingsAccountBonds.lt.100,SavingsAccountBonds.100.to.500,SavingsAccountBonds.500.to.1000,SavingsAccountBonds.gt.1000,SavingsAccountBonds.Unknown,EmploymentDuration.lt.1,EmploymentDuration.1.to.4,EmploymentDuration.4.to.7,EmploymentDuration.gt.7,EmploymentDuration.Unemployed,Personal.Male.Divorced.Seperated,Personal.Female.NotSingle,Personal.Male.Single,Personal.Male.Married.Widowed,Personal.Female.Single,OtherDebtorsGuarantors.None,OtherDebtorsGuarantors.CoApplicant,OtherDebtorsGuarantors.Guarantor,Property.RealEstate,Property.Insurance,Property.CarOther,Property.Unknown,OtherInstallmentPlans.Bank,OtherInstallmentPlans.Stores,OtherInstallmentPlans.None,Housing.Rent,Housing.Own,Housing.ForFree,Job.UnemployedUnskilled,Job.UnskilledResident,Job.SkilledEmployee,Job.Management.SelfEmp.HighlyQualified 6,1169,4,4,67,2,1,0,1,Good,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 48,5951,2,2,22,1,1,1,1,Bad,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 12,2096,2,3,49,1,2,1,1,Good,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,1,0,0 42,7882,2,4,45,1,2,1,1,Good,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,1,0 All data is now 5 rows only including the header. P.S. The tool takes a couple of seconds from 10 to 15 seconds to start working and compressing at the first time and then it runs faster and faster the more you use it.","title":"Using the lzhw Command Line tool"},{"location":"5%20Using%20the%20lzhw%20command%20line%20tool/#using-the-lzhw-command-line-tool","text":"In lzhw_cli folder, there is a python script that can work on command line tool to compress and decompress files without having to open it in python.","title":"Using the lzhw Command Line tool"},{"location":"5%20Using%20the%20lzhw%20command%20line%20tool/#lzhw-compression-tool","text":"Also a downloadable exe tool is available in this link . The tool allows to compress and decompress files from and to any form, csv,xlsx etc without any dependencies or installations. The tool now works perfectly on Windows and Mac version is being developed. Here is the file and its help argument to see how it works and its arguments: lzhw -h Output usage: lzhw_cli.py [-h] [-d] -f INPUT -o OUTPUT [-c COLUMNS [COLUMNS ...]] [-r ROWS] [-nh] LZHW is a tabular data compression tool. It is used to compress excel, csv and any flat file. Version: 0.0.8 optional arguments: -h, --help show this help message and exit -d, --decompress decompress input into output -f INPUT, --input INPUT input file to be (de)compressed -o OUTPUT, --output OUTPUT output where to save result -c COLUMNS [COLUMNS ...], --columns COLUMNS [COLUMNS ...] select specific columns by names or indices (1-based) to compress or decompress -r ROWS, --rows ROWS select specific rows to decompress (1-based) -nh, --no-header skip header / data to be compressed has no header As we can see, the tool takes an input file \"-f\" , and output \"-o\" where it should put the result whether it is compression or decompression based on the optional \"-d\" argument which selects decompression. The tool as well takes a \"-c\" argument which is the Columns in case we want only to compress or decompress specific columns from the input file instead of dealing with all the columns unnecessarily. This argument accepts names and indices seperated by coma. The \"-nh\" , --no-header, argument to specify if the data has no header. The \"-r\" , --rows, argument is to specify number of rows to decompress, in case we don't need to decompress all rows.","title":"LZHW Compression Tool"},{"location":"5%20Using%20the%20lzhw%20command%20line%20tool/#compress","text":"How to compress: The tool can be used through command line. For those who are new to command line, the easiest way to start it is to put the lzhw.exe tool in the same folder with the sheet you want to compress. Then go to the folder's directory at the top where you see the directory path and one click then type cmd , black command line will open to you where you can type the examples below. lzhw -f \"german_credit.xlsx\" -o \"gc_comp.txt\" Reading files, Can take 1 minute or something ... Running CScript.exe to convert xls file to csv for better performance Microsoft (R) Windows Script Host Version 5.812 Copyright (C) Microsoft Corporation. All rights reserved. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 647.30it/s] Creating gc_comp.txt file ... Compressed Successfully Let's say we are interested only in compressing the Age, Duration and Amount columns lzhw -f \"german_credit.xlsx\" -o \"gc_subset.txt\" -c Age,Duration,Amount Reading files, Can take 1 minute or something ... Running CScript.exe to convert xls file to csv for better performance Microsoft (R) Windows Script Host Version 5.812 Copyright (C) Microsoft Corporation. All rights reserved. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 249.99it/s] Creating gc_subset.txt file ... Compressed Successfully","title":"Compress"},{"location":"5%20Using%20the%20lzhw%20command%20line%20tool/#decompress","text":"Now it's time to decompress: If your original excel file was big and of many rows and columns, it's better and faster to decompress it into a csv file instead of excel directly and then save the file as excel if excel type is necessary. This is because python is not that fast in writing data to excel as well as the tool sometimes has \"Corrupted Files\" issues with excel. lzhw -d -f \"gc_comp.txt\" -o \"gc_decompressed.csv\" 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 690.45it/s] Creating gc_decompressed.csv file ... Decompressed Successfully Look at how the -d argument is used. Let's now check that it was decompressed really successfully: head -n 4 gc_decompressed.csv Duration,Amount,InstallmentRatePercentage,ResidenceDuration,Age,NumberExistingCredits,NumberPeopleMaintenance,Telephone,ForeignWorker,Class,CheckingAccountStatus.lt.0,CheckingAccountStatus.0.to.200,CheckingAccountStatus.gt.200,CheckingAccountStatus.none,CreditHistory.NoCredit.AllPaid,CreditHistory.ThisBank.AllPaid,CreditHistory.PaidDuly,CreditHistory.Delay,CreditHistory.Critical,Purpose.NewCar,Purpose.UsedCar,Purpose.Furniture.Equipment,Purpose.Radio.Television,Purpose.DomesticAppliance,Purpose.Repairs,Purpose.Education,Purpose.Vacation,Purpose.Retraining,Purpose.Business,Purpose.Other,SavingsAccountBonds.lt.100,SavingsAccountBonds.100.to.500,SavingsAccountBonds.500.to.1000,SavingsAccountBonds.gt.1000,SavingsAccountBonds.Unknown,EmploymentDuration.lt.1,EmploymentDuration.1.to.4,EmploymentDuration.4.to.7,EmploymentDuration.gt.7,EmploymentDuration.Unemployed,Personal.Male.Divorced.Seperated,Personal.Female.NotSingle,Personal.Male.Single,Personal.Male.Married.Widowed,Personal.Female.Single,OtherDebtorsGuarantors.None,OtherDebtorsGuarantors.CoApplicant,OtherDebtorsGuarantors.Guarantor,Property.RealEstate,Property.Insurance,Property.CarOther,Property.Unknown,OtherInstallmentPlans.Bank,OtherInstallmentPlans.Stores,OtherInstallmentPlans.None,Housing.Rent,Housing.Own,Housing.ForFree,Job.UnemployedUnskilled,Job.UnskilledResident,Job.SkilledEmployee,Job.Management.SelfEmp.HighlyQualified 6,1169,4,4,67,2,1,0,1,Good,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 48,5951,2,2,22,1,1,1,1,Bad,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 12,2096,2,3,49,1,2,1,1,Good,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,1,0,0 It looks awful in the command line :D but it's decompressed. Now let's say that we only interested in decompressing the first two columns that we don't remember how they were spelled. lzhw -d -f \"gc_comp.txt\" -o \"gc_subset_de.csv\" -c 1,2 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 5651.84it/s] Creating gc_subset_de.csv file ... Decompressed Successfully Now let's have a look at the decompressed file: head gc_subset_de.csv Duration,Amount 6,1169 48,5951 12,2096 42,7882 24,4870 36,9055 24,2835 36,6948 12,3059 We can also use the -r argument to decompress specific rows from the data frame. lzhw -d -f \"gc_comp.txt\" -o \"gc_subset_de.csv\" -r 4 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62/62 [00:00<00:00, 369.69it/s] Creating gc_subset_de.csv file ... Decompressed Successfully Here we only decompressed the firt 4 rows, 1-based, including the header. Let's look how the data looks like: cat \"gc_subset_de.csv\" Duration,Amount,InstallmentRatePercentage,ResidenceDuration,Age,NumberExistingCredits,NumberPeopleMaintenance,Telephone,ForeignWorker,Class,CheckingAccountStatus.lt.0,CheckingAccountStatus.0.to.200,CheckingAccountStatus.gt.200,CheckingAccountStatus.none,CreditHistory.NoCredit.AllPaid,CreditHistory.ThisBank.AllPaid,CreditHistory.PaidDuly,CreditHistory.Delay,CreditHistory.Critical,Purpose.NewCar,Purpose.UsedCar,Purpose.Furniture.Equipment,Purpose.Radio.Television,Purpose.DomesticAppliance,Purpose.Repairs,Purpose.Education,Purpose.Vacation,Purpose.Retraining,Purpose.Business,Purpose.Other,SavingsAccountBonds.lt.100,SavingsAccountBonds.100.to.500,SavingsAccountBonds.500.to.1000,SavingsAccountBonds.gt.1000,SavingsAccountBonds.Unknown,EmploymentDuration.lt.1,EmploymentDuration.1.to.4,EmploymentDuration.4.to.7,EmploymentDuration.gt.7,EmploymentDuration.Unemployed,Personal.Male.Divorced.Seperated,Personal.Female.NotSingle,Personal.Male.Single,Personal.Male.Married.Widowed,Personal.Female.Single,OtherDebtorsGuarantors.None,OtherDebtorsGuarantors.CoApplicant,OtherDebtorsGuarantors.Guarantor,Property.RealEstate,Property.Insurance,Property.CarOther,Property.Unknown,OtherInstallmentPlans.Bank,OtherInstallmentPlans.Stores,OtherInstallmentPlans.None,Housing.Rent,Housing.Own,Housing.ForFree,Job.UnemployedUnskilled,Job.UnskilledResident,Job.SkilledEmployee,Job.Management.SelfEmp.HighlyQualified 6,1169,4,4,67,2,1,0,1,Good,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 48,5951,2,2,22,1,1,1,1,Bad,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0 12,2096,2,3,49,1,2,1,1,Good,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,1,0,0 42,7882,2,4,45,1,2,1,1,Good,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,1,0 All data is now 5 rows only including the header. P.S. The tool takes a couple of seconds from 10 to 15 seconds to start working and compressing at the first time and then it runs faster and faster the more you use it.","title":"Decompress"}]}